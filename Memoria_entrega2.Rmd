---
title: "Practica 2"
author: "Carmen Liberal, Marcos López, Lara Montero"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: true
---

```{r echo=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(HDclassif))
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(gt))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(cluster))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library (NbClust))
suppressPackageStartupMessages(library (parameters))
suppressPackageStartupMessages(library (stats))
suppressPackageStartupMessages(library(summarytools))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(rpart.plot))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(naivebayes))
```

```{r echo = FALSE}
obesidad <- read.csv("C:/Users/laram/Downloads/archive (3)/ObesityDataSet.csv")
data <- dim(obesidad)
```

```{r echo = FALSE}
set.seed(1)

# Indices para la partición
nobesidad <- dim(obesidad)[1]
indices <- 1:nobesidad
ntrain <- nobesidad * 0.6
indices.train <- sample(indices, ntrain, replace = FALSE)
indices.test_val <- indices[-indices.train]

# Usamos el 60% para las variables de entrenamiento
train <- obesidad[indices.train, ]
test_val <- obesidad[indices.test_val, ]

# Creamos los indices para test y para val
ntest_val <- dim(test_val)[1]
indices = 1:ntest_val
ntest <-  ntest_val * 0.5
indices.test <- sample(indices, ntest, replace = FALSE)
indices.val <- indices[-indices.test]

# Creamos los datos de test y val
test <- test_val[indices.test, ]
val <- test_val[indices.val, ]

obesidad$Age <- round(obesidad$Age)
obesidad$NCP <- round(obesidad$NCP)
obesidad$FAF <- round(obesidad$FAF)

```


# 1. Planteamiento del Problema

Este conjunto de datos contienen la información para dar una estimación de los niveles de obesidad de personas residentes en Mexico, Perú y Colombia.

En este dataset podemos encontrar distintos tipos de variables:

* Gender: Genero. Variable discreta
* Age: Edad. Variable continua
* Height: Altura (m). Variable continua
* Weight: Peso (kg). Variablel continua
* family_history_with_overweight: Antecedentes familiares. Variable binaria ('yes', 'no')
* FAVC: Consumo frecuente de alimentos de alto valor genético. Variable binaria ('yes', 'no')
* FCVC: Frecuencia de consumo deverduras. Variable continua
* NCP: Número de comidas principales. Variable continua
* CAEC: Consumo de alimentos entre comidas. Variable discreta
* CH2O: Consumo de agua diariamente. Variable continua
* CALC: Consumo de alcohol. Variable discreta  
* SCC: Monitoreo del consumo de calorías. Variable binaria ('yes', 'no')
* FAF: Frecuencias de actividad física. Variable continua
* TUE: Tiempo utilizado dispositivos electronicos. Variable continua
* MTRANS: Transporte utilizado con frecuencia. Variable discreta
* NObeyesdad: Grupos segun el nivel de obesidad, variable discreta se diferencian en:

Dado nuestro problema, que es averiguar el nivel de obesidad de las personas según ciertas variables, buscamos crear un modelo de aprendizaje supervisado para poder averiguar de la forma más precisa el nivel de obesidad que tiene o que puede desarrollar una persona.


# 2. Entrenamiento de modelos y programación


## 2.1 Modelos


### k-NN

Primero vamos a ver que variable son las necesarias para poder hacer un buen modelo de knn. Como hemos visto en el EDA, hay ciertas variables que tiene más relación que otras, esas son height, weight, family history with overweight, gender, FAVC (Consumo de alimentos altos en calorias), FAF (Frecuencia de actividad física) y MTRANS (Medio de transporte).

```{r}
# Primero de todo, vamos a usar 10 fold cross validation y lo vamos a repetir 5 veces.
trainControl <- trainControl(method="repeatedcv", number=10, repeats=5)
metric <- "Accuracy"

# Creamos la semilla y buscamos el mejor valor de k para tener un buen modelo.
set.seed(1)

train.df <- train |> dplyr::select(Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad)
train.df[,2] <- scale(train.df[,2])
train.df[,3] <- scale(train.df[,3])
train.df[,6] <- scale(train.df[,6])

fit.knn <- train(NObeyesdad ~ ., data=train.df, method="knn", metric=metric ,trControl=trainControl, preProcess = c("center", "scale"))
knn.k1 <- fit.knn$bestTune 
print(fit.knn)
```
Como podemos observar, el mejor valor de k que podemos obtener es de k = 5. Al mirar los resultados podemos ver que el programa ha probado con k = 5 obteniendo una precisión del 80%, aproximadamente, y un indice kappa de 0.76 aproximadamente. Conforma aumentamos el valor de k, en este caso prueba con k = 7 y k = 9, podemos ver como la precisión de nuestro modelo disminuye, al igual que el indice kappa, lo que nos da a entender que cuanto más aumentemos k partiendo de k = 5, pero va a ser nuestro modelo. A continuación vamos a verlo en tabla.

```{r}
plot(fit.knn)
```

Como vemos a partir de k = 5, la precisión de nuestro modelo desciende draticamente. Por lo que viendo el gráfico y las métricas anteriores, podemos observar que k = 5 es el mejor valor que podemos seleccionar. A continuación vamos a obtener la predicción para nuestro conjunto de datos de prueba y vamos a imprimir la matriz de confusión. 

```{r}
set.seed(1)

# Primero calculamos la media de las variables que queremos estandarizar.
# media_height <- mean(train$Height)
# stddev_height <- sqrt(var(train$Height))
# test.df <- test |> mutate(Height=(Height - media_height)/stddev_height)
# 
# media_weight <- mean(train$Weight)
# stddev_weight <- sqrt(var(train$Weight))
# test.df <- test |> mutate(Weight=(Weight - media_weight)/stddev_weight)
# 
# media_FAF <- mean(train$FAF)
# stddev_FAF <- sqrt(var(train$FAF))
# test.df <- test |> mutate(FAF=(FAF - media_FAF)/stddev_FAF)

test.df <- test |> dplyr::select(Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad)
test.df[,2] <- scale(test.df[,2])
test.df[,3] <- scale(test.df[,3])
test.df[,6] <- scale(test.df[,6])

prediction <- predict(fit.knn,newdata=test.df)
cf <- confusionMatrix(prediction, as.factor(test.df$NObeyesdad),positive="yes")
print(cf)
```

### Análisis Discriminante Lineal.

Primero de todo, vamos a ver nuestra variable objetivo para ver si es de tipo factor, en caso de no serlo, lo transformamos.

```{r}
str(train$NObeyesdad)
```

Como es de tipo caracter, hacemos el cambio a factor

```{r}
train$NObeyesdad <- as.factor(train$NObeyesdad)
```

Ahora vamos a hacer el análisis discriminante lineal.

```{r}
# Creamos el modelo lda
lda_modelo <- lda(NObeyesdad ~ ., data = train)

# Buscamos las predicciones de lda
lda_predicciones <- predict(lda_modelo)

# Representacion del lda
lda_train <- data.frame(Componente_1 = lda_predicciones$x[, 1], Clase = train$NObeyesdad)

ggplot(lda_train, aes(x = Componente_1, y = 0, color = Clase)) +
  geom_point(size = 3) +
  labs(
    title = "Proyección de LDA",
    x = "Componente discriminante 1",
    y = ""
  ) +
  theme_minimal() +
  scale_color_manual(values = rainbow(length(unique(lda_train$Clase))))
```

Al visualizar la proyección del LDA, podemo ver que en la parte central de las componentes discriminantes hay tipos de obesidad solapados, lo que nos puede indicar que tienen caracteríasticas similares, entre ellos se pueden ver overweight level II y obesity type I, también se puede ver como tenemos valores que están en los extremos, lo que nos puede decir que diferenciarlos de los otros grupos es más senciello. Para estudiar mejor sus diferencias, vamos a hacer un boxplot de las componenetes por clase y un violin plot.

```{r}
# Definir colores
colores <- rainbow(length(unique(lda_train$Clase)))

# Boxplot
boxplot_lda <- ggplot(lda_train, aes(x = Clase, y = Componente_1, fill = Clase)) +
  geom_boxplot(alpha = 0.7) +
  labs(
    title = "Boxplot de la Proyección de LDA",
    x = "Clase",
    y = "Componente Discriminante 1"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    legend.position = "none" # Ocultar leyenda aquí
  ) +
  scale_fill_manual(values = colores)

# Violin Plot
violinplot_lda <- ggplot(lda_train, aes(x = Clase, y = Componente_1, fill = Clase)) +
  geom_violin(alpha = 0.7) +
  labs(
    title = "Violin Plot de la Proyección de LDA",
    x = "Clase",
    y = "Componente Discriminante 1"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    legend.position = "none" # Ocultar leyenda aquí también
  ) +
  scale_fill_manual(values = colores)

# Mostrar gráficos
grid.arrange(boxplot_lda, violinplot_lda, ncol = 2)
```

### Árboles de decisión

Este método es bastante fácil de interpretar, maneja tanto datos numéricos como categóricos, es bastante efectivo cuando las relaciones entre las variables no son lineales y nos ayuda a entender como cada característica afecta las decisiones del modelo, aunque puede acabar sobreajustando. 

En resumen, es un buen método de aprendizaje no supervisado para nuestro conjunto de datos.

```{r}
set.seed(1)
df <- train %>%
      dplyr::select(Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad)
fit.dt <- rpart(NObeyesdad~., data = df, method = 'class', control = rpart.control(maxdepth = 3))
rpart.plot(fit.dt, 
           extra = 106,
           cex = 0.6,
           type = 2,                      
           box.palette = "RdYlBu",
           shadow.col = "gray",
           main = "Árbol de Decisión",
           box.col = "lightblue",
           faclen = 2)
```

Hemos reducido la profundidad y por tanto la compliejidad del modelo y aumentada la explicabilidad.

Vamos a explicar el diagrama de arbol desde el nodo raíz hasta los nodos hoja:

El árbol empieza en el nodo raíz separando en 2 nodos dependiendo de la condición de si el peso es mayor de 100 (Obesidad tipo 3) o menor de 100 (peso normal), representando cada nodo el 33% y el 66% de los datos de entrenamiento respectivamente.

- Rama izquierda(67%): hay un 38% de probabilidad de que tengan peso insuficiente (y dentro de ese tipo, si la altura es menor de 1.70cm, hay una probabilidad de 53% que representan un 14% de los individuos con peso normal) y un 17% de que tengan algun tipo de sobrepeso (39% de sobrepeso nivel 1 y un 3% de sobrepeso nivel 2, representando el 17% y el 29% de los individuos de nuestro conjunto de datos de entrenamiento respectivamente)

- Rama derecha: Si el individuo es hombre, el árbol predice que tiene obesidad tipo 3 representando el 15% de los datos. Si no es hombre, se evalúa la condición de si pesa mas de 110 kg (obesidad tipo 2) o menos de 110 kg (obesidad tipo 1) representando el 12% y el 6% de los datos de entrenamiento.

En la rama derecha podemos ver que las probabilidades son de 0. Esto nos dice que hay un porcentaje muy bajo de individuos en nuestros datos que tienen algún tipo de obesidad.

```{r}
prediction <- predict(fit.dt, train, type = 'class')
cf <- confusionMatrix(prediction, as.factor(train$NObeyesdad),positive="yes")
print(cf)
```

Con la matriz de confusión podemos determinar que el método de árboles de decisión es muy buena, acertando en la mayoría de los casos el tipo de obesidad de los individuos y obteniendo un valor cercano a 1 en Pos Pred Value, Neg Pred Value, Specificity, Sensitivity y Balanced Accuracy en las estadísticas por clase.


### Métodos de Ensamblado

Aplicando métodos de ensamblado, podemos mejorar significativamente el rendimiento de predicción, ya que se basa en la unión de múltiples modelos, comenzaremos aplicando el esamblado de bagging.

#### Bagging

```{r}
rf <- randomForest(as.factor(NObeyesdad)~., data=train, importance=TRUE,proximity=TRUE) 
print(rf)
```

En cada división interna del árbol, se consideran 4 variables predictoras, lo que contribuye a reducir el sobreajuste. Como podemos observar, se muestra un error OOB de tan solo el 5,45%, esto nos indica una muy buena generalización del modelo.

Además, en la matriz de confusión se muestra como la gran mayoria de las clases de obesidad se clasifican con alta precisión. Todas las clases de obesidad tienen una tasa de error parecida y baja, destacan Obesidad Tipo 1, cuya tasa de error es del 0%, clasificándose correctamente en todos los casos. Por otro lado, Sobrepeso Nivel 1 es la que mayor tipo de error presenta, de un 16,86%, lo que sugiere que es el que mayores disficultades presenta a la hora de clasificar.

Continuamos evaluando visualmente la convergencia del modelo, para anlizar el comportamiento del modelo según se agregan árboles al conjunto.

```{r}
plot(rf)
```

La línea continua muestra el error promedio general, y el resto de líneas discontinuas son los errores según el tipo de obesidad. Algunos errores son muy cercanos a 0, podemos deducir que el rosa (el más bajo) corresponde a Obesidad Tipo 1 ya que su error era de era del 0% y el más elevado a Sobrepeso Nivel 1, ya que su error era el más alto. El error disminuye de manera rápida en las primeras iteraciones y a partir de los 200 árboles se estabiliza, esto demuestra una buena capacidad de generalización del modelo.

```{r}
# sobre la partición de prueba
#df.test <- train %>%
      #dplyr::select(Gender, Age, Height, Weight, family_history_with_overweight, FAVC, FCVC, NCP, CAEC, SMOKE, SCC, FAF, TUE, CH2O, CALC, MTRANS, #NObeyesdad)
#prediction.rf <- predict(rf, df.test,type="prob")[,2]
#clase.pred.rf=ifelse(prediction.rf>0.5,"yes","no")
#cf <- confusionMatrix(as.factor(clase.pred.rf), as.factor(df.test$NObeyesdad),positive="yes")
#print(cf)
```

A continuación, identificamos las variables predictoras que más influyen en las decisiones del modelo:

```{r}
importance(rf)
```

Esta función nos muestra dos medidas clave:

Mean Decrease Accuracy: Muestra cuánto disminuye la precisión del modelo excluimos cada variable.

Mean Decrease Ginni: Mide la pureza de los nodos que aporta cada variable al dividir los datos.

Por lo que según los resultados que hemos obtenido podemos deducir que la variable Weight es la más determinante para predecir el nivel de obesidad, ya que sus valores son los mas altos, su Mean Decrease Accuracy es de 126 y su Mean Decrease Ginni es de 389. Esto tiene total sentido, ya que el nivel de obesidad va ligado al peso y es coherente que sea la variable mas determinante para predecirlo.

Junto al peso, tenemos las variables Altura y Edad, que también son de las más importantes ya que sus valores son los siguientes mas elevados, y por ello también son variables muy relevantes al predecir el nivel de obesidad. También cabe descatar la importancia de ciertas variables como family_history_with_overweight y FCVC, sus valores también son relevantes y esto nos indica que el historial familiar de obesidad y la frecuencia en el consumo de vegetales también tienen una contribución relevante en el modelo.

Por el contrario, variables como SMOKE O SCC, presentan valores muy bajos e incluso negativos, lo que nos sugiere que son poco relevantes a la hora de predecir el nivel de obesidad. Lo cual concuerda con conclusiones anteriores en las que afirmamos que la variable fumar no afecta al nivel de obesidad.

Evaluamos la importancia de las variables:

```{r}
varImpPlot(rf)
```

Al representar graficamente las Mean Decrease Accurancy y Mean Decrease Gini podemos observar de manera más visual como la variables más importante es el peso, lo cual tiene sentido ya que el nivel de obesidad depende del peso. En ambas métricas destacan también de manera muy notable la altura y la edad. Además la frecuencia en el consumo de verduras, el consumo de agua (CH2O) y la frecuencia del consumo de comida calórica (CAEC), también son relevantes, lo que nos puede llevar a concluir la importancia de los hábitos alimenticios en el peso. Por último y coincidencio con lo anterior SCC Y SMOKE vuelven a tener el impacto menos notable en la predicción del modelo.

```{r}
MDSplot(rf,as.factor(train$NObeyesdad),k=3)
```

#### Boosting

A diferencia del Bagging, que entrena múltiples modelos en paralelo y los promedia, boosting entrena múltiples modelos en serie, es decir, **de manera secuencial**.

Haciendolo de esta manera, se da mayor peso a aquellas observaciones mal clasificadas en cada iteracción para intentar que el siguiente modelo aprenda a clasificar bien esas observaciones.

Vamos a aplicar el modelo XGBoost.

```{r}
# Convertimos los datos a numéricos
df.train <- map_df(train, function(columna) {
  columna %>% 
    as.factor() %>% 
    as.numeric %>% 
    { . - 1 }
})

datos <- list()
datos$train <- df.train

df.test <- map_df(test, function(columna) {
  columna %>% 
    as.factor() %>% 
    as.numeric %>% 
    { . - 1 }
})

datos$test <- df.test


# Convertimos los datos al formato Dmatrix
datos$train_mat  <- 
  datos$train %>% 
  dplyr::select(-NObeyesdad) %>% 
  as.matrix() %>% 
  xgboost::xgb.DMatrix(data = ., label = datos$train$NObeyesdad)
datos$test_mat  <- 
  datos$test %>% 
  dplyr::select(-NObeyesdad) %>% 
  as.matrix() %>% 
  xgboost::xgb.DMatrix(data = ., label = datos$test$NObeyesdad)

# Parámetros
params <- list(
  objective = "multi:softmax",  # Porque nuestra variable de NObeyesdad es multiclase
  num_class = 7,                # Numero de clases
  eval_metric = "mlogloss"
)

datos$modelo_01 <- xgboost(params= params, 
                           data = datos$train_mat,  
                           nround = 10,
                           max_depth=2, 
                           eta =0.3,
                           nthread =2)

xgb.plot.multi.trees(model = datos$modelo_01)
```

Vamos a explicar el árbol.

La parte superior del árbol está a la izquierda y la parte inferior a la derecha. Los números que salen al lado de cada variable es la calidad, la importancia de esa característica en el árbol.

Entonces, es muy fácil ver que la característica más importante de todas es con diferencia, Weight, el peso de cada individuo.


```{r}
importance_matrix <- xgb.importance(model = datos$modelo_01)

xgb.plot.importance(importance_matrix)
```

Gracias a este histograma, observamos que las características más importantes en orden de mayor a menor importancia son el peso, el género, la frecuencia de consumo de verduras, la altura y la edad, pero principalmente el peso del indivuduo, como era de esperar puesto que está muy relacionada con la variable NObeyesdad, que es la variable que tenemos que predecir.

```{r}
preds <- predict(datos$modelo_01, datos$test_mat)
accuracy <- mean(preds == datos$test$NObeyesdad)

cat(sprintf("Accuracy en test: %.2f%%\n", accuracy * 100))
```

Finalmente, el rendimiento de este modelo de Boosting es de 16% aproximadamente, lo que no nos es un modelo útil para nuestro conjunto de datos.


En conclusión, los resultados obtenidos del algoritmo implementado es mucho más eficiente (valor de Accuracy de 40% aproximadamente) ya que las iteraciones y bucles son controladas, el propio algoritmo está personalizado a nuestro caso concreto y hemos ajustado los parámetros para nuestro ejemplo concreto.


### Random Forest

Es una técnica de ensamblado que mejora la precisión de las predicciones. Se construye un bosque formado por varios árboles de decisión individuales no necesariamente correlacionados. Vamos a explicar un poco como funciona el algorítmo:

1. Iniciación: Definir los parámetros (número de árboles, tamaño de los nodos, etc)

2. Creación de cada árbol: Tomar una muestra bootstrap de tamaño n del conjunto de entrenamiento y crear un árbol usando esa muestra.
  - Selección de variables: seleccionar un pequeño conjunto de las variables.
  - Cálculo de la mejor división: para las variables seleccionadas, escoger la mejor variable y la mejor división.
  - División del nodo: Dividir el nodo en 2 nodos hijos en función de la variable seleccionada y la mejor división.
  - Recursión: Repetir el proceso de división recursivamente para cada nodo hijo hasta que se alcance un criterio de detención.
 
3. Output y división: devolver el ensamblado de árboles para hacer una predicción total

```{r}
rf <- randomForest(as.factor(NObeyesdad)~., data=train, importance=TRUE,proximity=TRUE) 
plot (rf)
legend("topright", 
       legend = colnames(rf$err.rate), 
       col = 1:ncol(rf$err.rate), 
       lty = 1, 
       cex = 0.8)
```

Vamos a ver paso a paso el gráfico que nos devuelve la funcion randomForest.

Nos demuestra el error frente al número de arboles durante la creación de los árboles desde 0 hasta 500. Cada linea discontinua representa cada valor (Insufficient_Weight, Normal_Weight, etc) y, la línea continua en negro representa el error medio.

Como era de esperar, al principio tiene un error muy alto que luego se va suavizando, pero una de ellas, Overweight_Level_II parece ser que su error no baja. Esto nos da a entender que es la variable más problematica para predecir y que no es fácil para el algoritmo prodecir su valor.

```{r}
# sobre la partición de prueba
df.test <- test %>%
      dplyr::select(Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad, Age, FCVC, NCP, CAEC, SMOKE, CH2O, SCC, TUE, CALC)
#df.test$NObeyesdad=as.factor(df.test$NObeyesdad)
prediction.rf <- predict(rf, df.test)
#clase.pred.rf=ifelse(prediction.rf>0.5)
cf <- confusionMatrix(as.factor(prediction.rf), as.factor(df.test$NObeyesdad))
print(cf)
```

Vamos a ver la matriz de confusión. 

Parece ser que tiene una muy buena capacidad de predicción, con un valor de Accuracy de 0.93 y un valor de Kappa de 0.92, es decir, muy buena capacidad de predicción

Vamos a visualizar ahora las variables que tienen más importancia dentro de nuestro modelo

```{r}
varImpPlot(rf)
```

Parece ser que la más importante es Weigth, la variable de peso del individuo que supera a simple vista todas las variables. Y detrás le siguen Height, la altura y Age, la edad

Esto es llamativo ya que en muchos de los modelos anteriormente usados concueran en que para predecir, la variable con más peso en la mayoría de los modelos es Weigth, principalmente.




### Naive Bayes

A continuación, vamos a aplicar el método Naive Bayes que estima la clase de obesidad mas probable para cada observación asumiendo independencia entre las variables.

Además, aplicamos Laplace Smoothing (laplace = 1), para evitar que algunas combinaciones de valores tengan probabilidad 0 y así no anulemos la probabilidad total de una clase.

```{r}
train$NObeyesdad <- as.factor(train$NObeyesdad)
test$NObeyesdad <- as.factor(test$NObeyesdad)

naive_model <- naive_bayes(NObeyesdad ~ ., data = train, usekernel = TRUE, laplace = 1)
pred_classes <- predict(naive_model, newdata = test)

# Matriz de confusión
confusionMatrix(pred_classes, test$NObeyesdad)

```

Tras aplicarlo, obtenemos una exactitud del 72,5% y un índice Kappa de 0,6775, lo que significa que el modelo tiene una buena capacidad para predecir las clases de obesidad. De hecho, Obesidad de tipo lll, está identificada con una precisión perfecta, sin embargo las clases que peor se ajustan son Sobrepeso Nivel l con sensibilidad del 50% y Sobrepeso Nivel ll con sensibilidad del 54%. Esto tiene sentido si nos fijamos en el análisis anterior usando el método Bagging, en el cual Sobrepeso Nivel l también era la que peor se ajustaba probablemente también por un solapamiento con Sobrepeso Nivel ll.


