---
title: "Practica 2"
author: "Carmen Liberal, Marcos López, Lara Montero"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: true
---

```{r echo=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(HDclassif))
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(gt))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(cluster))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library (NbClust))
suppressPackageStartupMessages(library (parameters))
suppressPackageStartupMessages(library (stats))
suppressPackageStartupMessages(library(summarytools))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(rpart.plot))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(naivebayes))
suppressPackageStartupMessages(library(fastDummies))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(recipes))
suppressPackageStartupMessages(library(mlbench))
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(MVN))
suppressPackageStartupMessages(library(lmtest))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(biotools))
suppressPackageStartupMessages(library(MLmetrics))
```

```{r echo = FALSE}
obesidad <- read.csv("C:/Users/laram/Downloads/archive (3)/ObesityDataSet.csv")
data <- dim(obesidad)
```

```{r echo = FALSE}
set.seed(1)

# Indices para la partición
nobesidad <- dim(obesidad)[1]
indices <- 1:nobesidad
ntrain <- nobesidad * 0.6
indices.train <- sample(indices, ntrain, replace = FALSE)
indices.test_val <- indices[-indices.train]

# Usamos el 60% para las variables de entrenamiento
train <- obesidad[indices.train, ]
test_val <- obesidad[indices.test_val, ]

# Creamos los indices para test y para val
ntest_val <- dim(test_val)[1]
indices = 1:ntest_val
ntest <-  ntest_val * 0.5
indices.test <- sample(indices, ntest, replace = FALSE)
indices.val <- indices[-indices.test]

# Creamos los datos de test y val
test <- test_val[indices.test, ]
val <- test_val[indices.val, ]

obesidad$Age <- round(obesidad$Age)
obesidad$NCP <- round(obesidad$NCP)
obesidad$FAF <- round(obesidad$FAF)

```


# 1. Planteamiento del Problema

Este conjunto de datos contienen la información para dar una estimación de los niveles de obesidad de personas residentes en Mexico, Perú y Colombia.

En este dataset podemos encontrar distintos tipos de variables:

* Gender: Genero. Variable discreta
* Age: Edad. Variable continua
* Height: Altura (m). Variable continua
* Weight: Peso (kg). Variablel continua
* family_history_with_overweight: Antecedentes familiares. Variable binaria ('yes', 'no')
* FAVC: Consumo frecuente de alimentos de alto valor genético. Variable binaria ('yes', 'no')
* FCVC: Frecuencia de consumo deverduras. Variable continua
* NCP: Número de comidas principales. Variable continua
* CAEC: Consumo de alimentos entre comidas. Variable discreta
* CH2O: Consumo de agua diariamente. Variable continua
* CALC: Consumo de alcohol. Variable discreta  
* SCC: Monitoreo del consumo de calorías. Variable binaria ('yes', 'no')
* FAF: Frecuencias de actividad física. Variable continua
* TUE: Tiempo utilizado dispositivos electronicos. Variable continua
* MTRANS: Transporte utilizado con frecuencia. Variable discreta
* NObeyesdad: Grupos segun el nivel de obesidad, variable discreta se diferencian en:

Dado nuestro problema, que es averiguar el nivel de obesidad de las personas según ciertas variables, buscamos crear un modelo de aprendizaje supervisado para poder averiguar de la forma más precisa el nivel de obesidad que tiene o que puede desarrollar una persona.


# 2. Entrenamiento de modelos y programación


## 2.1 Modelos


### k-NN

En esta sección se analiza el algoritmo de clasificación k-Nearest Neighbors (k-NN), con el objetivo de predecir el nivel de obesidad que puede alcanzar una persona, utilizando variables personales como edad, altura o peso, así como información sobre sus hábitos alimentarios, de actividad física y estilo de vida.

El algoritmo k-NN se basa en la idea de que una observación será similar a aquellas que tiene más cerca en el espacio de características. Es un modelo no paramétrico, lo que significa que no hace suposiciones sobre la distribución de los datos, y es especialmente útil cuando se desea aplicar una lógica de clasificación intuitiva basada en similitud.

En el contexto de este problema de clasificación multiclase —donde la variable objetivo NObeyesdad puede tomar siete valores distintos— k-NN es una opción válida, ya que maneja bien múltiples clases sin necesidad de modificar su estructura interna.

Antes de entrenar el modelo, se ha llevado a cabo un preprocesamiento imprescindible: se han codificado las variables categóricas mediante técnicas como one-hot encoding o codificación ordinal, y se han escalado todas las variables numéricas para que ninguna tenga un peso desproporcionado al calcular las distancias. Estas transformaciones son esenciales, ya que k-NN se basa en una métrica de distancia (normalmente euclídea) que es sensible a la escala de los datos.

```{r}
# Transformamos nuestra variable objetivo en factor.
train$NObeyesdad <- as.factor(train$NObeyesdad)
test$NObeyesdad <- as.factor(test$NObeyesdad)

# Creamos una funcón para normalizar las variables.
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Seleccionamos las variables que vamos a necesitar para el modelo.
train.df <- dplyr::select(train, Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad)
test.df <- dplyr::select(test, Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad)
val.df <- dplyr::select(val, Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad)

# Primero de todo, vamos a crear una receta para asegurarnos de que train, test y val sean consistentes, para poder tener las mismas variables en todas y que no halla problemas.
receta <- recipe(NObeyesdad ~ ., data = train.df) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_scale(all_numeric()) %>%
  step_zv(all_predictors()) %>%
  step_nzv(all_predictors())

# One-Hot Encoding con train, test y validation
train_prep <- prep(receta, training = train.df)
train.df <- bake(train_prep, new_data = train.df)
test.df <- bake(train_prep, new_data = test.df) 
val.df <- bake(train_prep, new_data = val.df)
```

Antes de hacer cualquier modelo, vamos a observar como se ven los datos, depues de la transformación que hemos hecho, vamos a comprobar que todo está bien y que no hay que ajustar nada. Para hacer esto, vamos a ver el resumen de train.df y test.df.

```{r}
summary(train.df)
```

```{r}
summary(test.df)
```
Ahora que tenemos todas las variables en train y test vamos con el modelo. Dado que el modelo de k-NN es una métrica de distancias, hemos cuantificado las variables categóricas, al igual que hemos escalado mediante la normalización min manx nuestras variables. A continuación, nos vamos a dedicar a buscar el mejor valor de k para hacer un modelo de Machine Learning que nos prediga bastante bien y con el que no tengamos problemas como sobreajuste. Para seleccionar el mejor valor de k, vamos a utilizar la técnica de grid search.

```{r}
# Usamos 10-fold cross validation repetida 3 veces
trainControl <- trainControl(method = "repeatedcv", 
                             number = 10, 
                             repeats = 3, 
                             classProbs = TRUE, 
                             summaryFunction = multiClassSummary,
                             savePredictions = "final")

set.seed(1)

# Introducimos el frid para buscar el mejor valor para k
grid <- expand.grid(.k = seq(1, 30, by=2))

fit.knn <- train(NObeyesdad ~ ., 
                 data = train.df, 
                 method = "knn", 
                 metric="Accuracy", 
                 tuneGrid = grid, 
                 trControl=trainControl)

knn.k2 <- fit.knn$bestTune
print(fit.knn)
```

```{r}
ggplot(fit.knn) +
  labs(title = "Precisión del modelo k-NN según el valor de k",
       x = "Número de vecinos (k)", y = "Accuracy")

```

Como podemos ver tanto en la gráfica como en el modelo, el mejor k posible es 1, sin embargo este valor de k es bastante bajo lo que nos puede indicar que tenemos cierto sobreajuste en nuestros datos, para poder verificar esto, vamos a ver como nos predice el modelo para los datos de entrenamiento y para los datos de test.

```{r}
prediction_train <- predict(fit.knn,newdata=train.df)
cf <- confusionMatrix(prediction_train, as.factor(train.df$NObeyesdad),positive="yes")
print(cf)
```

```{r}
prediction_test <- predict(fit.knn,newdata=test.df)
conf_matrix <- confusionMatrix(prediction_test, test.df$NObeyesdad)
print(conf_matrix$byClass[, c("F1", "Recall", "Precision")])  # Por clase
cat("F1-Macro:", mean(conf_matrix$byClass[, "F1"]), "\n")  # Global
```
```{r}
ggplot(as.data.frame(conf_matrix$table), aes(Prediction, Reference, fill = Freq)) +
  geom_tile() + 
  scale_fill_gradient(low = "white", high = "red") +
  labs(title = "Matriz de Confusión (k-NN) - ¡Sangre de Clases Mal Clasificadas!")
```


Viendo esta matriz de confusión y bajo la hipótesis que hemos dicho antes, podemos ver como el modelo ajusta bien para los datos con los que hemos entrenado el modelo, esto no es bueno, ya que para datos nuevo no predice bien, ya que el modelo se ajusta también al ruido de nuestros datos de entrenamiento. Como observamos, despues de probar el modelo con datos nuevos, hay bastante diferencia entre el accuracy de la prediccion con los datos de entrenamiento y los datos de testeo, por lo que podemos ver que al elegir el mejor k con grid search, nos lleva al sobreajuste, por lo que ahora vamos a hacer el modelo pero cambiando la métrica a F1. Para evitar esto, vamos a obligar a nuestro modelo a que no seleccione el valor de k = 1.



Dado que con F1 tambien nos sale un k mayor a 1, lo que vamos a hacer es forzar a nuestro problema a tener un k > 1, sin embargo tiene que ser lo suficiente grande para no tener sobreajuste, pero lo suficiente pequeño para que el modelo sea lo más probable posible, para ello, vamos a empezar en k = 5 y que sean impares.

```{r}
set.seed(1)

# Introducimos el frid para buscar el mejor valor para k
grid <- expand.grid(.k = seq(5, 30, by=2))

fit.knn <- train(NObeyesdad ~ ., 
                 data = train.df, 
                 method = "knn", 
                 metric="Mean_F1", 
                 tuneGrid = grid, 
                 trControl=trainControl)

knn.k2 <- fit.knn$bestTune
print(fit.knn)
```

```{r}
ggplot(fit.knn) +
  labs(title = "Precisión del modelo k-NN según el valor de k",
       x = "Número de vecinos (k)", y = "Mean_F1")
```

Despues de ajustarlo para que el valor mínimo sea k = 5, vemos que nos sale el valor de k = 5, ahora lo que vamos a hacer es comprobar que no hay sobreajuste en nuestros datos.

```{r}
prediction_train <- predict(fit.knn,newdata=train.df)
cf <- confusionMatrix(prediction_train, as.factor(train.df$NObeyesdad),positive="yes")
print(cf)
```

```{r}
prediction_test <- predict(fit.knn,newdata=test.df)
cf <- confusionMatrix(prediction_test, as.factor(test.df$NObeyesdad),positive="yes")
print(cf)
```

Despues ver como de preciso es nuestro modelo con los datos de entrenamiento, hemos podido observar que, nuestro modelo con datos nuevos tiene una exactitud del 80%, lo que, a priori, es bastante bueno, además de esto, tenemos que el p-valor es inferior a 0.05 lo que nos podría indicar que el modelo es significativo para poder hacer predicciones sobre nuestra variable objetivo, por otro lado, hay distintos valores que nos interesa estudiar y conocer:

Por un lado tenemos la sensibilidad, siendo esta la probabilidad de que a una persona se le asigne a una clase cuando de verdad se le asigna a esa clase, como podemos observar no tienen un valor excesivamente alto a excepcion de Obesity_Type_III que nos da una sensibilidad del 100%.

A parte, también tenemos la especificidad que es la probabilidad de predecir que una persona no pertenece a una de las clases de nuestra variable objetivo cuando de verdad no pertenece a esa clase. Como podemos ver, este modelo predice mejor la tasa de verdaderos negativos o especificidad que la sensibilidad, lo que nos da un modelo que nos predice más si una persona no pertenece a una clase que si una persona pertence a una clase.

También tenemos la precisión, que es el porcentaje de acertar cuando se predice una de las clases que tenemos en nuestra variable, como podemos ver hay algunas como Normal_Weight y Overweight_Level_II que tiene un valor predictivo positivo bastante bajo en comparacion con el modelo, lo que puede hacer que a la hora de predecir el modelo clasifique pero estas características.

Además de detectar los valores predictivos positivos, tambien puede detectar los negativos, que como era de esperar, ya que la especificidad es bastante alta, tiene valores bastante altos.

Por último tenemos la exactitud balanceada, con la que podmos ver con que exactitud nuestro modelo ajusta los datos en cada clase, como ya hemos dicho antes, la clase de Obesity_Type_III ajusta muy bien, dandonos una exactitud balanceada del 100%, mientras que los otrso tienen un valor que se acercamás al 80% a excepción de Overweight_Level_I y Overweight_Level_II que se acerca más al 80%. Para esto, vamos a ver las clases problemáticas.

```{r}
f1_scores <- confusionMatrix(prediction_test, test.df$NObeyesdad)$byClass[, "F1"]
print(f1_scores[f1_scores < 0.7])
```

Por lo que podemos concluir que es un modelo que ajusta bastatnte bien nuestros datos, sin embargo hay ciertas clases que le cuesta un poco más identificar, para trabajos futuros, lo que podrías buscar es poder aumentar la precisión de esas variables, manteniendo la explicabilidad del modelo y la exactitud, o por lo menos que no disminuya demasiado, nuestro modelo.

A continuación vamos a comparalo con el k-NN que hemos hecho manualmente.

```{r}
source("Memoria_Funciones.R")

train.df_x <- train.df |> dplyr::select(where(is.numeric))
test.df_x <- test.df |> dplyr::select(where(is.numeric))

pred_manual <- mi_knn(train = train.df_x, test = test.df_x, cl = train.df$NObeyesdad, k = 5)
table(Predicho = pred_manual, Real = test.df$NObeyesdad)

# Precisión manual
knn_manual <- mean(pred_manual == test.df$NObeyesdad)
```

```{r}
cf$table
```


### Análisis Discriminante Lineal.

A continuación vamos a crear un modelo aplicando el análisis discriminante lineal, con el cual lo que buscamos es la proyección de nuestros datos en un espaio de menor dimensionalidad donde tengamos clases que estén lo más separadas que se pueda, es decir, buscamos maximizar la distancia entre observaciones. Además de esto, este modelo asume normalidad multivariante y homocedasticidad, la varianza constante entre clases.

Dado que nuestro problema es conseguir diferenciar a personas por su tipo de obesidad, este modelo es adecuado ya que es adecuado cuando esas clases se diferencian bien, además de esto nos proporciona funciones discriminantes para interpretar varaibles que nos separan mejor las distintas clases de nuestra variables objetivo.

Pero hay que tener en cuenta ciertas cosas que nos pueden perjudicar como que este modelo es sensible para outliers además de que requiere que verifiquemos los supuestos antes mencionados, normalidad y homocedasticidad. Para ello, vamos a confirmar el supuesto de normalidad de multivariante mediante la prueba de Mardia

```{r}
train_num <- train.df %>% dplyr::select(is.numeric)
result <- mvn(data = train_num, mvnTest = "mardia")
result$multivariateNormality
```

Despues de comprovar, podemos ver como el p-vaor en ambos es inferior a 0.05, lo que nos indica que no hay normalidad multivariante, lo que nos dificulta el pode utilizar el lda, sin embargo, lo vamos a usar, ya que aunque no haya normalidad multivariante para aplicarlo como un modelo lineal de referncia y poder compara su rendimiento con otros modelos. A parte de esto, vamos a ver si nuestros datos presentan homocedasticidad.

```{r}
boxM(train.df[, -which(names(train.df) == "grupo")], train.df$NObeyesdad)
```

Al hacer la prueba de homocedasticidad mediante la prueba de Box's M, hemos obtenido un p-valor igual a 1, lo que nos suguiere que nuestro modeo tiene homocedasticidad en los datos, indicandonos que las matrices de covarianza son bastante similares entre los distintos grupos, por lo que, ya que el supuesto de homocedasticidad se cumple, pero el de linealidad no, vamos a proceder a hacer el lda para, como he dicho antes, usarlo como un modelo lineal de referencia para otros modelo.

```{r}
# Entrenamos el modelo
modelo_lda <- lda(NObeyesdad ~ ., data = train.df)

# Hacemos las predicciones sobre test
pred_lda <- predict(modelo_lda, newdata = test.df)

# Calculamos la exactitud de nuestro modelo
acc_lda <- mean(pred_lda$class == test.df$NObeyesdad)
cat("Accuracy LDA con MASS:", acc_lda, "\n")

```

Al entrenar el modelo de lda, podemos ver como el nievel de accuracy, o la representacion de la proporciones observadas correctamente predichas para nuestro modelo es bastante alta, siendo, aproximadamente 0.9, lo que podemos intuir que es un modelo que separa bastante bien las clases.

```{r}
confusion <- table(Predicted = pred_lda$class, Actual = test.df$NObeyesdad)
print(confusion)
```

Despues de crear el model y ver su accuracy, vamos a ver la matriz de confución para ver como de bien es capaz de predecir nuestro modelo. Como podemos observar, nuestro modelo precide bastante bien la mayoría de las clases de nuestra variable objetivo, a excepción de Overweight_Level_II, por lo que este modelo puede ser una buena comparación para otros modelos. A parte de esto, los distintos errore que vemos que puede cometer, podemos ver que son bastante normales, ya que son los de la clase siguiente o anterior, para ello, lo vaos a ver gráficamente.

```{r}
lda_train <- data.frame(Componente_1 = pred_lda$x[, 1], Clase = test.df$NObeyesdad)

ggplot(lda_train, aes(x = Componente_1, y = 0, color = Clase)) +
  geom_point(size = 3) +
  labs(
    title = "Proyección de LDA",
    x = "Componente discriminante 1",
    y = ""
  ) +
  theme_minimal() +
  scale_color_manual(values = rainbow(length(unique(lda_train$Clase))))
```

Como hemos podido ver en la matriz de confusión las distintas clases del tipo de obesidad se pueden ver bastante bien, además de poder verse como colosiona una clase con otra, confundiendose en parte de clase, por lo que puede ser un buen modelo para poder compararlo con otros. A continuación vamos a ver los distintos discriminantes y los vamos a nalizar.

```{r}
proj <- data.frame(
  LD1 = pred_lda$x[, 1],
  Clase = test.df$NObeyesdad
)

ggplot(proj, aes(x = LD1, fill = Clase)) +
  geom_density(alpha = 0.5) +
  labs(title = "Proyección LDA (primer discriminante)",
       x = "LD1", y = "Densidad") +
  theme_minimal()

```

En este primer discriminante, podemos ver como las distintas clases se separan perfectamente, además podemosv er que la clase Overweight_Level_II tiene la mayor densidad dentro de estas clases.

```{r}
proj <- data.frame(
  LD2 = pred_lda$x[, 2],
  Clase = test.df$NObeyesdad
)

ggplot(proj, aes(x = LD2, fill = Clase)) +
  geom_density(alpha = 0.5) +
  labs(title = "Proyección LDA (segundo discriminante)",
       x = "LD2", y = "Densidad") +
  theme_minimal()
```

En este segundo discriminante, se ve que las clases estan más juntas, incluso superpuestas, sin embargo vemos que las clases de Obesity_Type_II y Obesity_Type_III estan bastante separadas, además de tener las mayores densidades lo que puede indicar que para este discriminante son las más importantes.

```{r}
proj <- data.frame(
  LD3 = pred_lda$x[, 3],
  Clase = test.df$NObeyesdad
)

ggplot(proj, aes(x = LD3, fill = Clase)) +
  geom_density(alpha = 0.5) +
  labs(title = "Proyección LDA (tercer discriminante)",
       x = "LD3", y = "Densidad") +
  theme_minimal()
```

En este tercer discriminante, podemos ver como todas las clases están más o menos centradas, además de que se podría decir que siguien una distribución normal de media 0 (aproximadamente), por lo que podemos concluir que par este discriminante las variables se han normalizado.

```{r}
proj <- data.frame(
  LD4 = pred_lda$x[, 4],
  Clase = test.df$NObeyesdad
)

ggplot(proj, aes(x = LD4, fill = Clase)) +
  geom_density(alpha = 0.5) +
  labs(title = "Proyección LDA (cuarto discriminante)",
       x = "LD4", y = "Densidad") +
  theme_minimal()
```

En este discriminante, se pueden ver distintos grupos, en los cuales en ambos hay de todos los tipos de clases, podemos ver que en este discriminante lo que se ha hecho ha sido separa según dos parámetros que no sabemos exactamente cuales son.

```{r}
proj <- data.frame(
  LD5 = pred_lda$x[, 5],
  Clase = test.df$NObeyesdad
)

ggplot(proj, aes(x = LD5, fill = Clase)) +
  geom_density(alpha = 0.5) +
  labs(title = "Proyección LDA (quinto discriminante)",
       x = "LD5", y = "Densidad") +
  theme_minimal()
```
Este discriminante no sabemos muy bien que es lo que puede significar, para ello, necesitariamos al experto que nos ayudase a poder comprender esta nueva variable para poder hacer mejores predicciones.

```{r}
proj <- data.frame(
  LD6 = pred_lda$x[, 6],
  Clase = test.df$NObeyesdad
)

ggplot(proj, aes(x = LD6, fill = Clase)) +
  geom_density(alpha = 0.5) +
  labs(title = "Proyección LDA (sexto discriminante)",
       x = "LD6", y = "Densidad") +
  theme_minimal()
```

Con esta nos pasa lo mismo que con el discriminante anterior. A continuación vamos a ver las medidas que necesitamos para poder compara los modelos.

```{r}
predicciones <- as.factor(pred_lda$class)
test.df$NObeyesdad <- as.factor(test.df$NObeyesdad)

# Calculamos las métricas
resultados <- confusionMatrix(predicciones, test.df$NObeyesdad, positive = "ClasePositiva")  # Especificar la clase positiva
print(resultados)
```

Con estas medidas, podemos ver bastantes cosas muy interesantes, primero de todo es el nivel de accuracy, que como ya habiamos visto antes tiene un valor de 0.89, lo que nos da bastante explicabilidad, además de que el p-vamor es lo suficiente pequiño como para que el modelo sea bastante bueno.

Por oro lado tenemos las distintas mediad que podemos ver como en la sensibilidad, la tasa de veces que acertamos cuando tenemso que acertar la clase es bastante alta, estnado en la mayoría con un valor caso de 0.9, lo que nos puede indicar que puede diferenciar bien las clases. 

Pro otro lado, tenemos la tasa de verdaderos negativos, que es incluso mejor, como podemos ver el valor mínimo es de 0.95, lo que nos indica que sabe difernciar bien si una observacion va en una clase o no, dandonos un buen modelo de clasificación.

Por otro lado los valores predichos, tanto positivos como negativos nos dan bastante altos, dandonos bastante explicabilidad a los datos. Por último tenemos la precisión balancead inidcandonos la exactitud con la que se predice en cada clase, por lo que se ve es bastante alta, lo que nos dice que es bastante buena.

Sin embargo, aunque tengamos un muy buen modeo de clasificación no cumple el supuesto de normalidad multivariante que tiene que cumplir para poder aplicar lda, lo que nos da un modelo bastante bueno para compara con otros. 

Para concluir, vamos a comparar el modelo que hemos creado a manos siguiendo la siguiente fórmula:
$$
\delta_{c}(\mathbf{x})=\mathbf{x}^{t} \Sigma^{-1}\mu_{c} -\frac{1}{2}\mu_{c}^{t}\Sigma^{-1}\mu_{c} + \log(P(Y=c))
$$

```{r}
source("Memoria_Funciones.R")
train_num <- train.df |> dplyr::select(where(is.numeric))
test_num <- test.df |> dplyr::select(where(is.numeric))

# Entrena el modelo
modelo_lda <- mi_lda(X = train_num, y = train.df$NObeyesdad)

# Haz predicciones
pred_manual <- predict(modelo_lda, test_num)

# Matriz de confusión
table(Predicho = pred_manual, Real = test.df$NObeyesdad)

# Precisión manual
lda_manual <- mean(pred_manual == test.df$NObeyesdad)
```
```{r}
print(confusion)
```

Como podemos ver en la comparación, el modelo hecho a mano predice de forma similar al que hemos hecho con lda de r.

### Árboles de decisión

Este método es bastante fácil de interpretar, maneja tanto datos numéricos como categóricos, es bastante efectivo cuando las relaciones entre las variables no son lineales y nos ayuda a entender como cada característica afecta las decisiones del modelo, aunque puede acabar sobreajustando. 

En resumen, es un buen método de aprendizaje no supervisado para nuestro conjunto de datos.

```{r}
set.seed(1)
df <- train %>%
      dplyr::select(Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad)
fit.dt <- rpart(NObeyesdad~., data = df, method = 'class', control = rpart.control(maxdepth = 3))
rpart.plot(fit.dt, 
           extra = 106,
           cex = 0.6,
           type = 2,                      
           box.palette = "RdYlBu",
           shadow.col = "gray",
           main = "Árbol de Decisión",
           box.col = "lightblue",
           faclen = 2)
```

Hemos reducido la profundidad y por tanto la compliejidad del modelo y aumentada la explicabilidad.

Vamos a explicar el diagrama de arbol desde el nodo raíz hasta los nodos hoja:

El árbol empieza en el nodo raíz separando en 2 nodos dependiendo de la condición de si el peso es mayor de 100 (Obesidad tipo 3) o menor de 100 (peso normal), representando cada nodo el 33% y el 66% de los datos de entrenamiento respectivamente.

- Rama izquierda(67%): hay un 38% de probabilidad de que tengan peso insuficiente (y dentro de ese tipo, si la altura es menor de 1.70cm, hay una probabilidad de 53% que representan un 14% de los individuos con peso normal) y un 17% de que tengan algun tipo de sobrepeso (39% de sobrepeso nivel 1 y un 3% de sobrepeso nivel 2, representando el 17% y el 29% de los individuos de nuestro conjunto de datos de entrenamiento respectivamente)

- Rama derecha: Si el individuo es hombre, el árbol predice que tiene obesidad tipo 3 representando el 15% de los datos. Si no es hombre, se evalúa la condición de si pesa mas de 110 kg (obesidad tipo 2) o menos de 110 kg (obesidad tipo 1) representando el 12% y el 6% de los datos de entrenamiento.

En la rama derecha podemos ver que las probabilidades son de 0. Esto nos dice que hay un porcentaje muy bajo de individuos en nuestros datos que tienen algún tipo de obesidad.

```{r}
prediction <- predict(fit.dt, train, type = 'class')
cf <- confusionMatrix(prediction, as.factor(train$NObeyesdad),positive="yes")
print(cf)
```

Con la matriz de confusión podemos determinar que el método de árboles de decisión es muy buena, acertando en la mayoría de los casos el tipo de obesidad de los individuos y obteniendo un valor cercano a 1 en Pos Pred Value, Neg Pred Value, Specificity, Sensitivity y Balanced Accuracy en las estadísticas por clase.

Medidas de rendimiento usadas en el modelo:

La matriz de confusión, sensibilidad y especificidad (cuán bien detecta y descarta cada clase) y el valor Accuracy. 

Estas métricas nos permiten ver el modelo desde varios ángulos: la matriz de confusión nos muestra con detalle sus aciertos y errores; la sensibilidad y la especificidad nos dicen qué tan bien identifica correctamente cada tipo de obesidad; el valor predictivo positivo y negativo nos da la confianza real que podemos tener en las predicciones y el valor Accuracy nos garantiza que el rendimiento global del modelo equilibre de manera justa la detección de casos con el descarte correcto de los no afectados.

Con todo esto no solo comprobamos que el modelo sea preciso, sino que lo haga de manera equilibrada y fiable.


### Métodos de Ensamblado

Aplicando métodos de ensamblado, podemos mejorar significativamente el rendimiento de predicción, ya que se basa en la unión de múltiples modelos, comenzaremos aplicando el esamblado de bagging.

#### Bagging

```{r}
rf <- randomForest(as.factor(NObeyesdad)~., data=train, importance=TRUE,proximity=TRUE) 
print(rf)
```

En cada división interna del árbol, se consideran 4 variables predictoras, lo que contribuye a reducir el sobreajuste. Como podemos observar, se muestra un error OOB de tan solo el 5,45%, esto nos indica una muy buena generalización del modelo.

Además, en la matriz de confusión se muestra como la gran mayoria de las clases de obesidad se clasifican con alta precisión. Todas las clases de obesidad tienen una tasa de error parecida y baja, destacan Obesidad Tipo 1, cuya tasa de error es del 0%, clasificándose correctamente en todos los casos. Por otro lado, Sobrepeso Nivel 1 es la que mayor tipo de error presenta, de un 16,86%, lo que sugiere que es el que mayores disficultades presenta a la hora de clasificar.

Continuamos evaluando visualmente la convergencia del modelo, para anlizar el comportamiento del modelo según se agregan árboles al conjunto.

```{r}
plot(rf)
```

La línea continua muestra el error promedio general, y el resto de líneas discontinuas son los errores según el tipo de obesidad. Algunos errores son muy cercanos a 0, podemos deducir que el rosa (el más bajo) corresponde a Obesidad Tipo 1 ya que su error era de era del 0% y el más elevado a Sobrepeso Nivel 1, ya que su error era el más alto. El error disminuye de manera rápida en las primeras iteraciones y a partir de los 200 árboles se estabiliza, esto demuestra una buena capacidad de generalización del modelo.


A continuación, identificamos las variables predictoras que más influyen en las decisiones del modelo:

```{r}
importance(rf)
```

Esta función nos muestra dos medidas clave:

Mean Decrease Accuracy: Muestra cuánto disminuye la precisión del modelo excluimos cada variable.

Mean Decrease Ginni: Mide la pureza de los nodos que aporta cada variable al dividir los datos.

Por lo que según los resultados que hemos obtenido podemos deducir que la variable Weight es la más determinante para predecir el nivel de obesidad, ya que sus valores son los mas altos, su Mean Decrease Accuracy es de 126 y su Mean Decrease Ginni es de 389. Esto tiene total sentido, ya que el nivel de obesidad va ligado al peso y es coherente que sea la variable mas determinante para predecirlo.

Junto al peso, tenemos las variables Altura y Edad, que también son de las más importantes ya que sus valores son los siguientes mas elevados, y por ello también son variables muy relevantes al predecir el nivel de obesidad. También cabe descatar la importancia de ciertas variables como family_history_with_overweight y FCVC, sus valores también son relevantes y esto nos indica que el historial familiar de obesidad y la frecuencia en el consumo de vegetales también tienen una contribución relevante en el modelo.

Por el contrario, variables como SMOKE O SCC, presentan valores muy bajos e incluso negativos, lo que nos sugiere que son poco relevantes a la hora de predecir el nivel de obesidad. Lo cual concuerda con conclusiones anteriores en las que afirmamos que la variable fumar no afecta al nivel de obesidad.

Evaluamos la importancia de las variables:

```{r}
varImpPlot(rf)
```

Al representar graficamente las Mean Decrease Accurancy y Mean Decrease Gini podemos observar de manera más visual como la variables más importante es el peso, lo cual tiene sentido ya que el nivel de obesidad depende del peso. En ambas métricas destacan también de manera muy notable la altura y la edad. Además la frecuencia en el consumo de verduras, el consumo de agua (CH2O) y la frecuencia del consumo de comida calórica (CAEC), también son relevantes, lo que nos puede llevar a concluir la importancia de los hábitos alimenticios en el peso. Por último y coincidencio con lo anterior SCC Y SMOKE vuelven a tener el impacto menos notable en la predicción del modelo.

```{r}
MDSplot(rf,as.factor(train$NObeyesdad),k=3)
```

```{r}
vars <- setdiff(names(train), "NObeyesdad")

df.test <- test[, vars]
for (v in vars) {
  if (is.factor(train[[v]])) {
    df.test[[v]] <- factor(df.test[[v]], levels = levels(train[[v]]))
  } else {
    df.test[[v]] <- as.numeric(df.test[[v]])
  }
}

#Hacemos predicción
prediction.rf <- predict(rf, df.test)

#Creamos factores con niveles iguales para comparación
niveles <- levels(train$NObeyesdad)
prediction.rf <- factor(prediction.rf, levels = niveles)
real.rf <- factor(test$NObeyesdad, levels = niveles)

# Matriz de confusión
cf <- confusionMatrix(prediction.rf, real.rf)
print(cf)

```


En el conjunto de entrenamiento, el modelo mostraba un rendimiento casi excelente, con un accuracy del 95%o más, y siendo los errores mínimos. Sin embargo, al ajustarlo al conjunto de prueba, aunque el desempeño siga siendo alto podemos observar una ligera caída y aunque sigue sugiriendo que generaliza bien, podría ester sobreajustando brevemente.

En cuanto a la medidas de rendimiento, indican que el modelo clasifica correctamente la mayoria de categorías, destacando obesidad de nivel 3, que alcanza valores perfectos de sensibilidad, especificidad y precisión. Sin embargo, clases como Sobre Peso de Nivel 2 y Peso Normal, presentan menor sensibilidad y precisión, por lo que el modelo tiene más dificultad para distinguir entre clases cercanaso con menos representación. En conjunto, balanced accuracy y F1_score confirman un rendimiento global sólido, aunque con margen de mejora en clases específicas.

Ahora vamos a comparar el método bagging con la función propia de R, con el método programado a continuación:

Cargamos la función desde el archivo de funciones:

```{r}
source("Memoria_Funciones.R")
```

```{r}
# Entrenamos el bagging manual con 500 árboles
modelo_bagging <- bagging_manual(train, "NObeyesdad", n_trees = 500)

# Obtenemos predicciones usando solo los árboles en los que la observación fue OOB
pred_oob <- prediccion_oob(modelo_bagging$modelos, modelo_bagging$oob, train)

# Quitamos los NA (observaciones que ningún árbol dejó fuera)
idx_validos <- !is.na(pred_oob)

# Evaluamos la precisión sobre las observaciones OOB
matriz <- confusionMatrix(pred_oob[idx_validos], train$NObeyesdad[idx_validos])

print(matriz)

```

Como podemos observar, el método bagging manual alcanza una precisión del 88,87% y un coeficiente Kappa de 0,8691 lo cual nos indica un buen rendimiento y una alta concordancia entre los valores reales y la predicción. Las clases con mejor desempeño son Obesidad de Tipo lll, que se ajusta perfectamente y Obesidad de Tipo 2, con una sensibilidad del 96%. Por otro lado, Sobre Peso NIvel l y y Nivel ll tienen precisiones mas bajas y se puede observar confusión entre ellas.

Si lo comparamos con el modelo randomForest de R, realizado con el mismo número de árboles este logar una mayor precisión general y reduce el error OOB al 5,13%. Además, clasifica las clases de obesidad con una mayor precisión y reduce los errores de Sobre Peso Nivel l, aunque siga siendo la que se clasifica con mas dificultad.

Por tanto, podemos concluir que el bagging programado manualmente es eficaz y ajusta bien, pero el modelo automático es mejor, ya que es mas exacto y está más equilibrado.


#### Boosting

A diferencia del Bagging, que entrena múltiples modelos en paralelo y los promedia, boosting entrena múltiples modelos en serie, es decir, **de manera secuencial**.

Haciendolo de esta manera, se da mayor peso a aquellas observaciones mal clasificadas en cada iteracción para intentar que el siguiente modelo aprenda a clasificar bien esas observaciones.

Vamos a aplicar el modelo XGBoost.

```{r}
# Convertimos los datos a numéricos
df.train <- map_df(train, function(columna) {
  columna %>% 
    as.factor() %>% 
    as.numeric %>% 
    { . - 1 }
})

datos <- list()
datos$train <- df.train

df.test <- map_df(test, function(columna) {
  columna %>% 
    as.factor() %>% 
    as.numeric %>% 
    { . - 1 }
})

datos$test <- df.test


# Convertimos los datos al formato Dmatrix
datos$train_mat  <- 
  datos$train %>% 
  dplyr::select(-NObeyesdad) %>% 
  as.matrix() %>% 
  xgboost::xgb.DMatrix(data = ., label = datos$train$NObeyesdad)
datos$test_mat  <- 
  datos$test %>% 
  dplyr::select(-NObeyesdad) %>% 
  as.matrix() %>% 
  xgboost::xgb.DMatrix(data = ., label = datos$test$NObeyesdad)

# Parámetros
params <- list(
  objective = "multi:softmax",  # Porque nuestra variable de NObeyesdad es multiclase
  num_class = 7,                # Numero de clases
  eval_metric = "mlogloss"
)

datos$modelo_01 <- xgboost(params= params, 
                           data = datos$train_mat,  
                           nround = 10,
                           max_depth=2, 
                           eta =0.3,
                           nthread =2)

xgb.plot.multi.trees(model = datos$modelo_01)
```

Vamos a explicar el árbol.

La parte superior del árbol está a la izquierda y la parte inferior a la derecha. Los números que salen al lado de cada variable es la calidad, la importancia de esa característica en el árbol.

Entonces, es muy fácil ver que la característica más importante de todas es con diferencia, Weight, el peso de cada individuo.


```{r}
importance_matrix <- xgb.importance(model = datos$modelo_01)

xgb.plot.importance(importance_matrix)
```

Gracias a este histograma, observamos que las características más importantes en orden de mayor a menor importancia son el peso, el género, la frecuencia de consumo de verduras, la altura y la edad, pero principalmente el peso del indivuduo, como era de esperar puesto que está muy relacionada con la variable NObeyesdad, que es la variable que tenemos que predecir.

```{r}
preds <- predict(datos$modelo_01, datos$test_mat)
accuracy <- mean(preds == datos$test$NObeyesdad)

cat(sprintf("Accuracy en test: %.2f%%\n", accuracy * 100))
```

Finalmente, el rendimiento de este modelo de Boosting es de 16% aproximadamente, lo que no nos es un modelo útil para nuestro conjunto de datos.


En conclusión, los resultados obtenidos del algoritmo implementado es mucho más eficiente (valor de Accuracy de 40% aproximadamente) ya que las iteraciones y bucles son controladas, el propio algoritmo está personalizado a nuestro caso concreto y hemos ajustado los parámetros para nuestro ejemplo concreto.

Medidas de rendimiento utilizadas:

La matriz de confusión que nos dice exactamente en qué grados de obesidad el modelo secuencial se equivoca, mostrando falsos positivos y negativos por clase.
El log-loss (“mlogloss”) que penaliza las predicciones muy seguras que resultan erróneas, ayudándonos a calibrar la confianza del modelo.
El Accuracy (de un 16%) que nos indica que XGBoost no generaliza bien el conjunto de test.
La sensibilidad y especificidad por clase nos dicen en qué niveles de obesidad detecta correctamente casos y evita falsos diagnósticos.
Los valores predictivos positivo y negativo nos aportan la confianza real en cada predicción concreta.


### Random Forest

Es una técnica de ensamblado que mejora la precisión de las predicciones. Se construye un bosque formado por varios árboles de decisión individuales no necesariamente correlacionados. Vamos a explicar un poco como funciona el algorítmo:

1. Iniciación: Definir los parámetros (número de árboles, tamaño de los nodos, etc)

2. Creación de cada árbol: Tomar una muestra bootstrap de tamaño n del conjunto de entrenamiento y crear un árbol usando esa muestra.
  - Selección de variables: seleccionar un pequeño conjunto de las variables.
  - Cálculo de la mejor división: para las variables seleccionadas, escoger la mejor variable y la mejor división.
  - División del nodo: Dividir el nodo en 2 nodos hijos en función de la variable seleccionada y la mejor división.
  - Recursión: Repetir el proceso de división recursivamente para cada nodo hijo hasta que se alcance un criterio de detención.
 
3. Output y división: devolver el ensamblado de árboles para hacer una predicción total

```{r}
rf <- randomForest(as.factor(NObeyesdad)~., data=train, importance=TRUE,proximity=TRUE) 
plot (rf)
legend("topright", 
       legend = colnames(rf$err.rate), 
       col = 1:ncol(rf$err.rate), 
       lty = 1, 
       cex = 0.8)
```

Vamos a ver paso a paso el gráfico que nos devuelve la funcion randomForest.

Nos demuestra el error frente al número de arboles durante la creación de los árboles desde 0 hasta 500. Cada linea discontinua representa cada valor (Insufficient_Weight, Normal_Weight, etc) y, la línea continua en negro representa el error medio.

Como era de esperar, al principio tiene un error muy alto que luego se va suavizando, pero una de ellas, Overweight_Level_II parece ser que su error no baja. Esto nos da a entender que es la variable más problematica para predecir y que no es fácil para el algoritmo prodecir su valor.

```{r}
# Calcula la clase (tipo) de cada variable
train_classes <- sapply(df.train, class)
test_classes  <- sapply(df.test,  class)

# Junta en un data.frame para ver las diferencias
class_comparison <- data.frame(
  variable = names(train_classes),
  train    = unname(train_classes),
  test     = unname(test_classes),
  stringsAsFactors = FALSE
)

# Solo para las variables que deberían ser factores
factor_vars <- c("Gender","family_history_with_overweight","FAVC","MTRANS",
                 "CAEC","SMOKE","SCC","NObeyesdad")

na_counts <- sapply(df.test, function(x) sum(is.na(x)))

# Seleccionar columnas
df.train <- train %>%
  dplyr::select(Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad, Age, FCVC, NCP, CAEC, SMOKE, CH2O, SCC, TUE, CALC)

df.test <- test %>%
  dplyr::select(Gender, Height, Weight, family_history_with_overweight, FAVC, FAF, MTRANS, NObeyesdad, Age, FCVC, NCP, CAEC, SMOKE, CH2O, SCC, TUE, CALC)

# Asegura clases idénticas
for(v in names(df.train)) {
  cls <- class(df.train[[v]])
  if (cls == "factor") {
    # refactoriza usando exactamente los mismos niveles
    df.test[[v]] <- factor(
      as.character(df.test[[v]]),
      levels = levels(df.train[[v]])
    )
  } else if (cls == "numeric") {
    df.test[[v]] <- as.numeric(df.test[[v]])
  } else if (cls == "integer") {
    df.test[[v]] <- as.integer(df.test[[v]])
  } else if (cls == "character") {
    df.test[[v]] <- as.character(df.test[[v]])
  } else if (cls == "logical") {
    df.test[[v]] <- as.logical(df.test[[v]])
  }
}

# Predecir y mostrar matriz de confusión
prediction.rf <- predict(rf, df.test)
cf <- confusionMatrix(prediction.rf, df.test$NObeyesdad)
print(cf)

```

Vamos a ver la matriz de confusión. 

Parece ser que tiene una muy buena capacidad de predicción, con un valor de Accuracy de 0.93 y un valor de Kappa de 0.92, es decir, muy buena capacidad de predicción

Vamos a visualizar ahora las variables que tienen más importancia dentro de nuestro modelo

```{r}
varImpPlot(rf)
```

Parece ser que la más importante es Weigth, la variable de peso del individuo que supera a simple vista todas las variables. Y detrás le siguen Height, la altura y Age, la edad

Esto es llamativo ya que en muchos de los modelos anteriormente usados concueran en que para predecir, la variable con más peso en la mayoría de los modelos es Weigth, principalmente.

Medidas de rendimiento utilizadas:

La matriz de confusión, Accuracy, coeficiente Kappa

La matriz de confusión nos muestra con detalle cuántos casos de cada categoría de obesidad el modelo acierta o confunde, revelando falsos positivos y falsos negativos. 
El valor Accuracy (0,93) indica que el bosque de árboles acierta en el 93 % de las predicciones, un nivel de acierto muy elevado. 
El coeficiente Kappa (0,92) que corrige la probabilidad de aciertos aleatorios y confirma que nuestro modelo clasifica de forma significativamente mejor que el azar.
La sensibilidad, que nos dice qué proporción de verdaderos casos, y los valores predictivos positivo y negativo.

En conjunto, estas métricas garantizan que nuestro modelo de Random Forest no solo sea preciso, sino también equilibrado y fiable.


### Naive Bayes

A continuación, vamos a aplicar el método Naive Bayes que estima la clase de obesidad mas probable para cada observación asumiendo independencia entre las variables.

Además, aplicamos Laplace Smoothing (laplace = 1), para evitar que algunas combinaciones de valores tengan probabilidad 0 y así no anulemos la probabilidad total de una clase.

```{r}
train$NObeyesdad <- as.factor(train$NObeyesdad)
test$NObeyesdad <- as.factor(test$NObeyesdad)

naive_model <- naive_bayes(NObeyesdad ~ ., data = train, usekernel = TRUE, laplace = 1)
pred_classes <- predict(naive_model, newdata = test)

# Matriz de confusión
confusionMatrix(pred_classes, test$NObeyesdad)

```

Tras aplicarlo, obtenemos una exactitud del 72,5% y un índice Kappa de 0,6775, lo que significa que el modelo tiene una buena capacidad para predecir las clases de obesidad. De hecho, Obesidad de tipo lll, está identificada con una precisión perfecta, sin embargo las clases que peor se ajustan son Sobrepeso Nivel l con sensibilidad del 50% y Sobrepeso Nivel ll con sensibilidad del 54%. Esto tiene sentido si nos fijamos en el análisis anterior usando el método Bagging, en el cual Sobrepeso Nivel l también era la que peor se ajustaba probablemente también por un solapamiento con Sobrepeso Nivel ll.

Analizando las medidas de rendimiento que se muestran para cada clase, la clase Peso insuficiente se desempeña correctamente, su sensibilidad, es decir la proporcion de casos correctamente clasificados es del 83,87%  y su especificidad es aun, mas alta, con un 99,17% por lo que distingue muy bien los que no pertenecen a esta clase. En el caso del peso Normal, aunque distingue bien los que no pertenecen a esta clase, sus sensibilidad es mas baja, 63,6% y la preicisón también lo es, 56%. Continuamos con obesidad de tipo 1, que detecta el 67% de los casos reales y tiene buena capacidad para evitar falsos positivos (especificidad 91,7%) aunque la precisión podría mejorar. En cuanto a obesidad de tipo ll, reconoce bien los casos reales (sensibilidad 81,8%)y no se equivoca mucho al predecir, (precisión 71,4% y especificidad 95,1%). Obesidad de tipo lll al tener un desempeño perfecto su valor en todas las métricas es del 100%. Sobre peso de Nivel 1 es un poco más folo, ya que solo detecta la mitad de los casos reales (sensibilidad 50%) y su precisión es solo del 69%. Por último, Sobre peso de Nivel 2 solo detecta el 54% de los casos y su precisión también es baja, (45,8%), aunque su especificidad es buena (91,4%)

A continuación, hemos implementado manualmente el algoritmo Naive Bayes y asi comprender su funcionamiento interno. Este modelo programado nos sirve como base conceptual pero para la predicción debemos usar el de la función de R. Ya que el rendimiento ha sido muy bajo, porque no incluye optimizaciones, control de errores o transformación de variables que si incluye la función de R:

```{r}
# Predicciones en TEST
modelo_naive_manual <- entrenar_naive_bayes(train, "NObeyesdad")
pred_test_manual <- sapply(1:nrow(test), function(i) {
  predecir_naive_bayes(modelo_naive_manual, test[i, ])
})

# Evaluación
confusion_test <- confusionMatrix(factor(pred_test_manual, levels = levels(test$NObeyesdad)),
                                  test$NObeyesdad)
print(confusion_test)

```


Al comparar el modelo Naive Bayes implementando la función propia de R (naivebayes) con la función programada podemos observar una diferencia muy significativa en el rendimiento. El modelo de R logra una precisión del 72,5% y un coeficiente Kappa de 0,6775, mientras que el modelo manual apenas alcanza un 11,6% de acierto y un Kappa practicamente nulo por lo que su rendimiento no supera al azar. En el modelo automático, hay clases que hemos observado anteriormente se clasifican con gran precisión, mientras que en el modelo manual casi ninguna clase se identifica correctamente, predominan las predicciones erróneas concentradas en la clase Peso Normal. Esta diferencia se debe a que el modelo de R optimiza automáticamente los cálculos, maneja variables de forma más robusta y controla mas la distribución y el suavizado.


# Comparación de Modelos

En este bloque vamos a determinar cuál de todos es el más adecuado, en cuantro a métricas, para predecir el nivel de obesidad de forma más precisa.

Para abordar el problema de clasificación del nivel de obesidad, hemos seleccionado tres familias de métodos que cubren, desde nuestro punto de vista, desde la sencillez y transparencia hasta la capacidad predictiva más avanzada.

Estos 3 modelos, que son los que mejores resultados en cuanto a rendimiento principalmente, son: k-Nearest Neighbors, Bagging y Random Forest. Cada uno basado en similitud, en ensamblado puro y en ensamblado con selección de variables.

Hemos escogido estos tres enfoques porque, en conjunto, cubren muy bien el compromiso entre sencillez, interpretabilidad y potencia predictiva

Vamos a empezar explicando en profundidad cada modelo.

## k-nn

### Por qué lo elegimos

- Es un método “no paramétrico” que no asume ninguna distribución de los datos.

- Clasifica según las observaciones más cercanas en el espacio de características, por lo que es fácilmente interpretable


### Ventajas

- Transparencia: fácil de entender y de implementar.

- Flexibilidad: maneja directamente problemas multiclase sin modificaciones.

- Poco ajuste de hiperparámetros: solo requiere elegir k (el número de vecinos).

### Desventajas

- Sensibilidad a la escala: requiere normalizar o estandarizar previamente.

- Coste computacional: crece con el tamaño del conjunto de entrenamiento.

- Propenso a sobreajuste si k es muy pequeño

## Bagging

### Por qué lo elegimos

- Permite visualizar el efecto de cada árbol a través de predicciones OOB (out-of-bag).

### Ventajas

- Reducción de varianza: disminuye el sobreajuste

- Fácil de implementar sin librerías especializadas.

- Visibilidad OOB: se evalúa el rendimiento usando sólo los datos que no entraron en cada muestra.

### Desventajas

- No reduce correlación entre árboles: todos se entrenan con todas las variables, por lo que pueden ser muy similares.

## Random Forest

### Por qué lo elegimos

- Mejora al bagging añadiendo selección aleatoria de variables en cada división.

- Combina reducción de varianza con menor correlación entre árboles.

### Ventajas

- Muy buena generalización sin excesivo ajuste de hiperparámetros.

- Estimación OOB incorporada para validación interna.

- Importancia de variables: detecta automáticamente qué características aportan más.

### Desventajas

- Menos interpretable que un árbol único o k-NN muy simple.

- Coste computacional superior al de un solo árbol, aunque paralelo.

## Rendimientos de los modelos

```{r echo=FALSE, message=FALSE, warning=FALSE}
predictoras <- setdiff(names(train), "NObeyesdad")
df_test_raw <- test[ , predictoras]

for(col in predictoras) {
  if (is.factor(train[[col]])) {
    df_test_raw[[col]] <- factor(df_test_raw[[col]], levels = levels(train[[col]]))
  }
}

# Predicciones
pred_knn <- predict(fit.knn, newdata = test.df)
preds_mat <- sapply(modelo_bagging$modelos, function(arbol) {
  predict(arbol, newdata = df_test_raw, type = "class")
})
pred_bagging <- apply(preds_mat, 1, function(votos) {
  names(which.max(table(votos)))
})
pred_bagging <- factor(pred_bagging, levels = levels(train$NObeyesdad))
pred_rf <- predict(rf, newdata = df_test_raw)

# Calcula las matrices de confusión
cm_knn     <- confusionMatrix(pred_knn,     test.df$NObeyesdad)
cm_bagging <- confusionMatrix(pred_bagging, test$NObeyesdad)
cm_rf      <- confusionMatrix(pred_rf,      test.df$NObeyesdad)

# Función para extraer métricas
get_metrics <- function(cm) {
  c(
    Accuracy    = unname(cm$overall["Accuracy"]),
    Kappa       = unname(cm$overall["Kappa"]),
    Sensitivity = mean(cm$byClass[ , "Sensitivity"], na.rm = TRUE),
    Specificity = mean(cm$byClass[ , "Specificity"], na.rm = TRUE)
  )
}

# Métricas de cada modelo
metrics_knn     <- get_metrics(cm_knn)
metrics_bagging <- get_metrics(cm_bagging)
metrics_rf      <- get_metrics(cm_rf)

comparativa <- bind_rows(
  as.data.frame(t(metrics_knn))     %>% mutate(Modelo = "k-NN (k = 5)"),
  as.data.frame(t(metrics_bagging)) %>% mutate(Modelo = "Bagging manual (500)"),
  as.data.frame(t(metrics_rf))      %>% mutate(Modelo = "Random Forest (500)")
)

comparativa <- as.data.frame(comparativa)[ , c("Modelo", "Accuracy", "Kappa",  "Sensitivity", "Specificity")]

kable(comparativa)
```


Vamos a comentar la tabla.

- k-nn: Obtiene un accuracy del 83.6 % y un Kappa de 0.81, lo cual es razonable para un modelo tan simple. La especificidad es muy alta (97.3 %), indicando que casi nunca etiqueta erróneamente como positiva una observación que no lo es. Sin embargo, la sensibilidad es algo más baja (82.1 %), lo que significa que falla en captar cerca del 18 % de las verdaderas instancias de cada clase.

- Bagging: Mejora el accuracy hasta el 88.6 % (+5 puntos frente a k-NN) y el Kappa a 0.87 (el ensamblado reduce significativamente el sesgo de k-NN). La sensibilidad sube a 87.99 % y la especificidad a 98.1 %. El modelo no sólo acierta más, sino que equilibra mejor la detección de positivos y la evitación de falsos positivos.

- Random Forest: Lidera con un accuracy del 94.8 % y un Kappa de 0.94, reflejando una capacidad predictiva casi perfecta. La sensibilidad alcanza el 94.6 %, capturando casi todas las verdaderas instancias, y la especificidad el 99.2 %, prácticamente eliminando falsos positivos.

En conclusión, Random Forest es el más equilibrado y potente, Bagging se queda unos puntos atrás y k-NN a pesar de ser el más rápido e iterpretable, sufre de sensibilidad.

Es por eso que el modelo más potente y del cual obtenemos un rendimiento y una capacidad predictiva casi perfecta es el Random Forest.

Ahora que ya hemos elegido el modelo que mejor se ajusta a nuestros datos de obesidad, vamos a evaluarlo en el conjunto validacion

```{r}
# Nos aseguramos de que validacion tenga las mismas columnas y tipos que train (excepto la clase)
for (col in names(train)) {
  if (col %in% names(val)) {
    if (is.factor(train[[col]])) {
      val[[col]] <- factor(val[[col]], levels = levels(train[[col]]))
    } else if (is.numeric(train[[col]])) {
      val[[col]] <- as.numeric(val[[col]])
    }
  }
}



# Predecimos
pred_rf <- predict(rf, newdata = val)
library(caret)

# Nos aseguramos de que val tenga los mismos niveles para la variable respuesta
val$NObeyesdad <- factor(val$NObeyesdad, levels = levels(train$NObeyesdad))

# predicciones
pred_rf <- predict(rf, newdata = val)

# Evaluar con matriz de confusión
conf_rf <- confusionMatrix(pred_rf, val$NObeyesdad)

# Mostrar resultados
print(conf_rf)


```

Evaluando el modelo en validación este alcanza un rendimiento excelente con unos valores de accuracy y Kappa que superan a los resultados obtenido en test.

```         
Accuracy : 0.9551
```

```         
Kappa : 0.9475           
```

La sensibilidad media es muy alta también, con un valor de 0,95, lo cual inidca identifica bien la mayoria de los casos en todas las clases.
La especificidad también destaca con un 99,4%, moestrando que el modelocasi no comete falsos positivos.

Comparado con los resultados en test, donde Random Forest ya era el más exacto, al evaluar en validación se ha reafirmado su fiabilidaz. Es claramente el más consistente frente a otros modelos, por lo que demuestra ser el modelo con el mejor rendimiento. Además, ofrece interpretabilidad mediante la importancia de variables, donde destacan el peso la altura y edad, lo que refuerza por tanto la coherencia del modelo sobre que estas variables son cruciales para clasificar los niveles de obesidad.

Tras analizar y modelar el conjunto de datos de obesidad, aplicar diversas técnicas, abordar distintos modelos y evaluar su rendimiento con distintas métricas, hemos podido sacar conclusiones sobre ellos. Los resultados a lo largo del trabajo han mostrado que las variables más determinantes para clasificar la edad son el peso, la altua y la edad, lo que concuerda con los conocimientos médicos acerca de la obesidad de hoy en dia. También hemos podido detectar como algunas clases de obesidad como Sobre Peso Nivel l y Nivel ll han presentado mas dificultades para clasificarse, por ser mas cercanas, mientras que Obesdiad de tipo 3 y Peso insuficiente se identifican con mucha más facilidad. En definitiva, tras aplicar todos estos modelos, técnicas y compararlas, destacando Random Forest que nos permite obtener las predicciones más precisas y generalizables, los resultados obtenidos han sido útiles y nos han permitido sacar conclusiones acerca de la obesidad que podrían aplicarse en entornos reales.



