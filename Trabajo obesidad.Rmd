---
title: "Obesidad"
author: "Marcos López García"
date: "2025-01-31"
output: html_document
---

```{r echo=FALSE}
library(readr)
library(ggplot2)
library(dplyr)
library(HDclassif)
library(MASS)
library(gt)
library(knitr)
library(cluster)
library (tidyverse)
library (factoextra)
library (NbClust)
library (parameters)
library (stats)
```

# Introducción

Este conjunto de datos contienen la información para dar una estimación de los niveles de obesidad de personas residentes en Mexico, Perú y Colombia.

Estos datos se han adquirido de [repositorio de datos de kaggle](https://www.kaggle.com/datasets/aravindpcoder/obesity-or-cvd-risk-classifyregressorcluster). Segun la descripción del repositorio, nos basamos en personas con edades entre 14 y 61 años y diversos hábitos alimenticios y condición física. Estos datos se han recolectado mediante una plataforma web con una encuesta donde usuarios anónimos respondieron cada pregunta.

En este dataset podemos encontrar distintos tipos de variables:

* Gender: Genero.
* Age: Edad.
* Height: Altura (m).
* Weight: Peso (kg).
* family_history_with_overweight: Antecedentes familiares.
* FAVC: Consumo frecuente de alimentos de alto valor genético.
* FCVC: Frecuencia de consumo deverduras.
* NCP: Número de comidas principales.
* CAEC: Consumo de alimentos entre comidas.
* CH2O: Consumo de agua diariamente.
* CALC: Consumo de alcohol.
* SCC: Consumo de calorías.
* FAF: Frecuencias de actividad física.
* TUE: Tiempo utilizado dispositivos electronicos.
* MTRANS: Transporte utilizado con frecuencia.
* NObeyesdad: Grupos segun el nivel de obesidad, se diferencian en:

  * Bajo peso: Menos de 18,5.
  * Normal: 18,5 a 24,9.
  * Sobrepeso: 25,0 a 29,9.
  * Obesidad I: 30,0 a 34,9.
  * Obesidad II: 35,0 a 39,9.
  * Obesidad III: Mayor de 40.

# Data understanding

Primero de todo, vamos a cargar los datos y vamos a ver la cantidad de variables y observaciones que encontramos.

```{r}
obesidad <- read.csv("ObesityDataSet.csv")
data <- dim(obesidad)
```
Podemos observar que tenemos una cantiadad de `r data[1]` observaciones y `r data[2]` variables en este dataset

En estos datos, las varibles son de distintos tipos:
```{r}
str(obesidad)
```

Podemos ver como hay distintos tipos de variables en los datos, en el cual solo hay dos tipos, el tipo numérico `num` (`Age`, `Height`, `Weight`, `FCVV`, `NCP`, `CH2O`, `FAF`Y `TUE`) y por otro lado, tenemos vectores de caracteres `char` (`Gender`, `family_history_with_overweight`, `FAVC`, `CAEC`, `SMOKE`, `SSC`, `CALC`, `MTRANS`, `NObeyesdad`)

Antes de hacer la partición de los datos para hacer un mejor análisis, vamos a observar si hay valores faltantes.
```{r}
colSums(is.na(obesidad))
```

Podemos observar que no hay variables de tipo NA, por lo que no es necesario hacer algún tipo de imputación sobre los datos para hacer el análisis de los datos.

Realizamos la partición de nuestros datos.
```{r}
set.seed(1)

# Indices para la partición
nobesidad <- dim(obesidad)[1]
indices <- 1:nobesidad
ntrain <- nobesidad * 0.6
indices.train <- sample(indices, ntrain, replace = FALSE)
indices.test_val <- indices[-indices.train]

# Usamos el 60% para las variables de entrenamiento
train <- obesidad[indices.train, ]
test_val <- obesidad[indices.test_val, ]

# Creamos los indices para test y para val
ntest_val <- dim(test_val)[1]
indices = 1:ntest_val
ntest <-  ntest_val * 0.5
indices.test <- sample(indices, ntest, replace = FALSE)
indices.val <- indices[-indices.test]

# Creamos los datos de test y val
test <- test_val[indices.test, ]
val <- test_val[indices.val, ]
```

Miramos la información de nuestros datos de entrenamiento
```{r}
dim(train)
head(train)
str(train)
```

# EDA

## Variable objetivo

Antes de nada vamos a mirar como se puede ver nuetra variable objetivo, que sería el nivel de obesidad que hay en esta muestra, primero vamos a ver información sobre la variable

```{r}
summary(train$NObeyesdad)
table(train$NObeyesdad)
```

Vamos a visualizar la distribución de las distintas categorías dentro de la variable objetivo NObesity, que se refiere a los distintos niveles de obesidad. Esto lo haremos mediante dos gráficos de barras, en horizontal.

```{r}
train |> 
  ggplot(aes(x = NObeyesdad, fill = NObeyesdad)) +
  geom_bar() +
  theme_bw() + 
  labs(title = "Tipos de obesidad", 
       x = "Tipo de obesidad",
       y = "Frecuencia absoluta") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Tal y como se puede observar en el gráfico, la frecuencia de cada tipo de obesidad es bastante parecida. Podemos ver que la clase que muestra una menor frecuencia es la de peso insuficiente, por otro lado, la clase con una mayor frecuencia es la de Obesidad de tipo I, seguido del peso normal (segunda mayor frecuencia) y de la obesidad de tipo III (tercera mayor frecuencia).Es decir, la categoría que encontramos con mayor frecuencia es obesidad de tipo I.

Comparamos el peso con la altura según el nivel de obesidad
```{r}
train |> ggplot(mapping = aes(x = Height, y = Weight, color = NObeyesdad)) +
  geom_point()
```

**Nivel de obesidad en función de la cantidad de comidas.**

Primero de todo vamos a observar la relación que puede haber entre el tipo de obesidad con el número de comidas que hacen las personas al día.

```{r}
train$FCVC = round(train$FCVC)

ggplot(train, aes(x = FCVC, fill = NObeyesdad , colour = NObeyesdad)) + 
  geom_histogram(position = "dodge", bins = 5)
```

Podemos ver que las personas que hacen una comida son menos propensos de desrrollar obesidad de algún tipo. Por otro lado, hay una cosa que nos parece curiosa y es qe al hacer tres comidas hay bastabtes personas que han desarrollado obesidad de tipo 3 o no tienen el suficiene peso. Viendo esta tabla se puede ver que las personas hacen sobre todo 3 comidas al día.

**Nivel de obesidad según el peso.**

Ahora representaremos los niveles de obesidad distribuidos según el peso mediante un box plot:

```{r}
train |> 
  ggplot(aes(x = NObeyesdad , y = Weight )) +  
  geom_boxplot() +
  theme_bw() +  
  labs(title = "Box-plot de Niveles de Obesidad por peso") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Como podemos observar en el BoxPlot, los niveles de obesidad estan fuertemente relacionados con el peso. Las personas con un mayor peso son las que padecen de Obesidad tipo 3, dentro de esta categoria podemos observar un valor atípico, que se sale por encima del rango, y seria la persona con mas peso entorno a 172,5 kg. La siguiente clase con mas peso son las Obesidad tipo 2, donde también hay ciertos valores atípicos que está vez están por debajo del rango. La tercera categoria con mas peso es la Obesidad tipo 1, donde encontramos al mayor número de personas si recordamos el gráfico anterior. Por último ordenadas de mayor a menor peso encontramos: Sobrepeso nivel 1, Sobrepeso nivel 2, Peso normal y Peso insuficiente.

**Boxplot de los niveles de obesidad distribuidos según la altura.**

```{r}
train |> 
  ggplot(aes(x = NObeyesdad , y = Height )) +  
  geom_boxplot() +
  theme_bw() +  
  labs(title = "Niveles de Obesidad por altura")  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Aunque algunas categoría presentan distribuciones más amplias no parece haber una diferencia muy grande entre la altura y los niveles de obesidad. Existen valores atípicos en las categorías Obesidad tipo ll (Obesity II ) y Sobrepeso tipo ll (Overweight ll), lo que significa que hay personas con alturas significativamente mas pequeñas a la mayoria de su grupo en el caso de Obesidad tipo ll y también mayores para Sobrepeso tipo ll

**Boxplot de los niveles de obesidad distribuidos según la edad.**

```{r}
train |> 
  ggplot(aes(x = NObeyesdad , y = Age )) +  
  geom_boxplot() +
  theme_bw() +  
  labs(title = "Box-plot de Nieveles de Obesidad por edad")  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Los niveles de menor peso, Insufficient Weight y Normal Weight, tienen una mediana de edad más baja, entorno a los 20-25 años. Obesity Type l, Obesity Type ll y Obesity Type lll, tienen medianas de edad mas altas, entorno a los 25 años. Podemos observar también que Overweight Level ll tiene una variabilidad mayor en la edad. Encontramos valores atípicos que sobrepasan en la mayoría de categorías. De manera clara podemos concluir que las personas con menor peso tienden a ser más jóvenes, y las personas con un nivel de obesidad mayor tienen una media de edad más alta.

**Nivel de obesidad en función de si fuma o no**

Comparamos la variable objetivo con la variable categórica fumar usando un gráfico de barras:

```{r}
table(train$SMOKE)

```

Como hay una gran diferencia entre el número de personas que fuman y las que no calculamos proporciones dentro de cada grupo para representar un gráfico de barras que muestra la proporción de personas que fuman y las que no dentro de cada tipo de obesidad.

```{r}
# Calcular proporciones dentro de cada grupo de SMOKE
data_prop <- train %>%
  group_by(SMOKE, NObeyesdad) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(SMOKE) %>%
  mutate(prop = count / sum(count) * 100)

ggplot(data_prop, aes(x = NObeyesdad, y = prop, fill = SMOKE)) +
  geom_bar(stat = "identity", position = "dodge") +  # Barras separadas para cada grupo
  labs(x = "Nivel de Obesidad", y = "Proporción (%)", fill = "¿Fuma?",
       title = "Proporción de Niveles de Obesidad según si Fuma o No") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Viendo esta tabla podemos ver que si las personas fuman no tiene una relación con el nivel de obesidad. Debido a que la mayoría de las personas que fuman se encuentran en obisdad de tipo II, sin embargo el siguiente grupo en el que las personas fuman son personas de peso norma. 

Como la dieta está directamente relacionada con el peso, analizaremos como ciertas variables relacionadas con la dieta afectan al peso:

La primera variables que analizaremos es la frecuencia de consumo de comidas con muchas calorias.

```{r}
table(train$FAVC)
```

De nuevo nos encontramos con una diferencia significable entre el número de personas que si consumen comidas con un alto nivel de calorias y las que no, por tanto realizaremos un gráfico de barras que muestra la proporción de personas que consumen este tipo de comidas o no dentro de cada nivel de obesidad.

```{r}
# Calculamos proporciones dentro de cada grupo de FAVC
data_prop <- train %>%
  group_by(FAVC, NObeyesdad) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(FAVC) %>%
  mutate(prop = count / sum(count) * 100)

ggplot(data_prop, aes(x = NObeyesdad, y = prop, fill = FAVC)) +
  geom_bar(stat = "identity", position = "dodge") +  # Barras separadas para cada grupo
  labs(x = "Nivel de Obesidad", y = "Proporción (%)", fill = "¿Consume frecuentemente comidas 
       con un nivel alto de calorias?",
       title = "Proporción de Niveles de Obesidad según si toma comidas con alto nivel de calorias o no") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Tal y como podemos observar, a medida que el nivel de obesidad es mayor la proporción de personas que no comen este tipo de comidas disminuye, recordemos que el orden es:

•Underweight •Normal •Overweight l\
•Overweight ll •Obesity I •Obesity II •Obesity III

Por tanto, podemos sacar como conclusión que el consumo de comidas con alto nivel de calorias efecta directamente al nivel de obesidad de la persona.

Continuaremos analizando otra variable que tiene que ver con la dieta, FCVC que es la frecuencia del consumo de vegetales. Como esta variable es continua, observaremos su distribución por nivel de obesidad de dos maneras mediante un histograma y un boxplot:

Realizaremos un histograma de cada nivel de obesidad y su ditribución del consumo de vegetales para que sea mas claro, ya que al ser varios niveles en un mismo histrograma no se ve con claridad.

```{r}
ggplot(train, aes(x = FCVC, fill = NObeyesdad)) +
  geom_histogram(binwidth = 0.5, alpha = 0.7, position = "identity") +
  facet_wrap(~NObeyesdad) +
  labs(x = "Frecuencia de Consumo de Vegetales", y = "Cantidad de Personas",
       title = "Distribución del Consumo de Vegetales según Nivel de Obesidad") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
train |> 
  ggplot(aes(x = NObeyesdad, y = FCVC)) +  
  geom_boxplot() +  
  theme_bw() +  
  labs(title = "Consumo de vegetales por NIvel de Obesidad") 
```

La siguiente variable será CAEC, que hace referencia a ''comer entre comidas", analizaremos

```{r}
table(train$CAEC)
```

```{r}
train |> 
  ggplot(aes(x = NObeyesdad, fill = CAEC)) +
  geom_bar(position = position_dodge()) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Ahora agrupamos la categoria Mtrans (que asigna el medio de transporte que usan las personas de la muestra) en los que van andando y los que van en transporte, para ver como afecta el movimiento a la obesidad.

```{r}


# Crear la nueva variable de transporte
obesidad_transp <- train %>%
  mutate(MTRANS_grouped = case_when(
    MTRANS %in% c("Automobile", "Motorbike", "Public_Transportation") ~ "No Walking",
    MTRANS %in% c("Walking", "Bike") ~ "Walking",
    TRUE ~ MTRANS # En caso de valores inesperados
  ))
```

Creamos un grafico para las personas que no van andando ni en bici y así poder visualizar su nivel de obesidad.

```{r}
#No Walking

ggplot(filter(obesidad_transp, MTRANS_grouped == "No Walking"), aes(x = NObeyesdad, fill = NObeyesdad)) +
  geom_bar(position = "dodge") +  # Barras separadas
  labs(x = "Nivel de Obesidad", y = "Frecuencia", fill = "NObesity") +
  ggtitle("Frecuencia de NObesity en personas que NO caminan") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Ahora, para las personas que andan, es decir que se mueven creamos otro gráfico para ver su nivel de obesidad.

```{r}
# Walking
ggplot(filter(obesidad_transp, MTRANS_grouped == "Walking"), aes(x = NObeyesdad, fill = NObeyesdad)) +
  geom_bar(position = "dodge") +  # Barras separadas
  labs(x = "Nivel de Obesidad", y = "Frecuencia", fill = "NObesity") +
  ggtitle("Frecuencia de NObesity en personas que caminan") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

El número de personas con obesidad dentro del grupo de las que no caminan es mucho mayor que en el grupo de las que sí caminan, ya que como se puede observar en el segundo gráfico (personas que sí caminan) el número de personas con peso normal (Normal_Weight) es muy elevado. Esto se ajusta y tiene sentido en la realidad, ya que el movimiento que una persona realiza en su día a día está directamente relacionado con tener un peso más o menos elevado.

```{r}

ggplot(filter(obesidad_transp, family_history_with_overweight == "yes"), aes(x = NObeyesdad, fill = NObeyesdad)) +
  geom_bar(position = "dodge") +  # Barras separadas
  labs(x = "Nivel de Obesidad", y = "Frecuencia", fill = "NObesity") +
  ggtitle("Frecuencia de NObesity en personas con Historial familiar de Sobrepeso") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
# Filtrar datos donde la persona NO tiene historial familiar de sobrepeso
ggplot(filter(obesidad_transp, family_history_with_overweight == "no"), aes(x = NObeyesdad, fill = NObeyesdad)) +
  geom_bar(position = "dodge") +  # Barras separadas
  labs(x = "Nivel de Obesidad", y = "Frecuencia", fill = "NObesity") +
  ggtitle("Frecuencia de NObesity en personas sin Historial familiar de Sobrepeso") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

El primer gráfico muestra los niveles de obesidad en personas con un historial familiar de obesidad, tal y como se puede observar, cuanto mayor es el nivel de obesidad, mas personas hay con familiares con obesidad. Ya que, las categorías "peso insuficiente" y "peso normal" son las que menos gente con este historial tienen, en las categorias de sobrepeso (Overweight) el número de personas con familiares con obesidad aumenta y ya por último en las de los tres tipos de obesidad, el número de mucho mayor.

En el segundo gráfico se muestra la otra categoria, pesonas sin historial familiar de sobrepeso. En este caso ocurre lo contrario, cuanto menor es el nivel de obesidad, mayor es la frecuencia con la no existe un historial familiar de sobrepeso. Cabe destacar que en los niveles de Obesidad 1 y 2, la frecuencia es casi nula y en el nivel de Obesidad 3 no hay frecuencia, por lo que no hay nadie con ese nivel de obesidad, que es el más alto, que no tenga un historial familiar de obesidad.

Por tanto, podemos concluir que tener un historial familiar de obesidad afecta directamente al nivel de obesidad de la persona, ya sea por genética o por el estilo de vida.

# **PCA**

## Con la función prcomp() de R:

```{r}
library(readr)
 ironman_lake_placid_female_2022 <- read_csv("~/Escritorio/aprendizaje/ironman_lake_placid_female_2022.csv")
```

Ahora trabajaremos con un conjunto de datos sobre participantes femeninas en un triatlon Ironman, las variables que utilizaremos son, Swim.Time, Run.Time y Bike.Time, mediante la función head podemos observar la estructura del conjunto de datos y sus variables:

```{r}
head(ironman_lake_placid_female_2022)
```

Resumen estadístico de cada variable:

```{r}
summary(ironman_lake_placid_female_2022)
```

Observamos la estructura de dataset:

```{r}
str(ironman_lake_placid_female_2022)
```

Como hemos mencionado previamente, las variables que usaremos para PCA son Swim.Time, Run.Time, y Bike.Time (tiempo en cada tipo de carrera), por lo tanto las seleccionamos:

```{r}
#Seleccionamos solo las variables necesarias para hacer el PCA, que son Swim.Time, Bike.Time y Run.Time
ironman_numeric <- ironman_lake_placid_female_2022[, c("Swim.Time", "Bike.Time", "Run.Time")]
```

```{r}
#Eliminamos filas con valores NA
ironman_numeric <- na.omit(ironman_numeric)

# Verificamos si hay valores NA
sum(is.na(ironman_numeric))

#Verificamos que no haya varianza cero en las columnas
ironman_numeric <- ironman_numeric[, sapply(ironman_numeric, function(col) var(col, na.rm = TRUE) > 0)]
```

Realizamos el análisis de componentes principales (PCA) que se basa en simplificar la estructura de datos, preservando al mismo tiempo la mayor cantidad posible de información. Este método convierte un grupo de variables originales en un conjunto nuevo de variables no correlacionadas llamadas componentes principales. Estas nuevas variables capturan la variabilidad presente en los datos de manera más eficiente.

Con la función prcomp realizamos el PCA:

```{r}
# Aplicar PCA con escalado
pca_result <- prcomp(ironman_numeric, scale. = TRUE)
```

```{r}
dim(pca_result$x)
```

Nuestro PCA nos da un total de 3 componentes principales (PC1, PC2 Y PC3) y 489 atletas

Visualizamos la importancia relativa de cada componente:

```{r}
pca_result <- prcomp(ironman_numeric, scale=T)

# Graficamos el PCA para observar la varianza que explica cada componente
plot(pca_result, col = c("coral", "brown", "pink"), 
     main = "Varianza Explicada por Componentes", 
     xlab = "Componentes Principales")


axis(1, at=1:3, labels=c("PC1", "PC2", "PC3"))

```

```{r}
summary(prcomp(ironman_numeric,scale=T))
```

Como podemos observar, la variabilidad de los datos la explica principalmente la componente principal PC1, ya que en el gráfico de barras es significativamente más alta que las demás. Además en la matriz Rotation PC1 explica el 56,71% de la varianza total. La segunda componente más influyente es PC2, que explica un 33,26%, con las dos primeras variables explicamos el 90% de la variabilidad. A la segunda componente principal la sigue PC3 con un 10%, por lo que esta última es la menos explicativa.

```{r}
#Desviaciones típicas de los autovalores
prcomp(ironman_numeric, scale=T)$sdev
#cantidad de variabilidad explicada por cada componente principal
```

Las desviaciones típicas explican también la cantidad de variabilidad explicada por cada componente principal y PC1 es la mayor y por tanto las que más explica.

Por tanto, las dos primeras componentes principales PC1 Y PC2, son suficientemente representativas como para realizar un gráfico de los datos con ellas, ya que explican la mayor parte de la varianza.

Continuamos representando la matriz de rotacion, cuyos valores muestran cuanto contribuye cada variable original a cada componente principal:

```{r}
# Rotation representa la matriz de rotacion
prcomp(ironman_numeric, scale=T)$rotation
```

PC1 asigna pesos a las variables y como se ve en la matriz todas tienen un peso con el mismo signo lo que significa que hace un promedio ponderado de las variables principales.

```{r}
plot(prcomp(ironman_numeric,scale=T)$x[,1:2], col = adjustcolor("cyan3"))
```

Representamos los datos en las dos primeras componentes principales, PC1 Y PC2. La mayoría de los datos están agrupados entorno al valor 0 de la segunda componente principal, por lo que parece que las diferencias entre los datos en la dimensión capturada por PC2 no son muy grandes. Sin embargo, nos encontramos con un valor excepcional el cual se encuentra por debajo del valor -20 de PC2, lo que sugiere que puede tratarse de un atleta con características númericas diferentes en algunas de las variables consideradas. Para analizarlo, vamos a representar los datos con el nombre de cada atleta:

```{r}
#Hacemos que ironman_numeric sea un dataframe para poder poner los nombres 
ironman_numeric <- as.data.frame(ironman_numeric)  
rownames(ironman_numeric) <- ironman_lake_placid_female_2022$Name

plot(prcomp(ironman_numeric,scale=T)$x[,1:2], type='n')
text(prcomp(ironman_numeric,scale=T)$x[,1:2],labels=rownames(ironman_numeric))
```

Como podemos leer, la atleta con valores extremos es Dominique Charron:

```{r}
ironman_lake_placid_female_2022[ironman_lake_placid_female_2022$Name == "Dominique Charron", c("Run.Time", "Bike.Time", "Swim.Time")]

```

```{r}
colMeans(ironman_lake_placid_female_2022[, c("Run.Time", "Bike.Time", "Swim.Time")], na.rm = TRUE)

```

Comparándolo con la media de las columnas, la media del valor de Swim.time es 88,1032 y sin embargo, el valor de Swim.Time de Dominique Charron es 3498, por lo que es un valor atípico y su punto en el gráfico se aleja significativamente del resto, vamos a probar a representarlo quitando a esta atleta:

```{r}
# Eliminar la fila 183 
ironman_numeric <- ironman_numeric[-183, ] 
plot(-prcomp(ironman_numeric, scale=T)$x[,1:2], col = adjustcolor("cyan3"))
```

Podemos observar una tendencia a la agrupación en la zona central, por lo que interpretamos que la mayoría de atletas tienen un rendimiento similar entre ellas. Sin embargo, también hay ciertos puntos más alejados de la zona central y por tanto más lejos del resto. Esto nos sugiere que hay atletas con un rendimiento diferenciable, pero sin valores extremadamente alejados como era el caso de Dominique Charron. No podemos observar una separación clara en distintos grupos, pero los valores más cercanos entre ellos tendrán un rendimiento similar.

Además, tal y como hemos visto en la matriz de rotación, como todas las variables originales, (Bike Time, Swim Time y Run Time) tienen pesos negativos en PC1, cuanto mas a la izquierda en el eje x, que es el correspondiente a PC1, mayor es su tiempo en las disciplinas. Por lo tanto, los valores mas a la izquierda corresponden a atletas mas lentos, veamos cuales son los más destacados:

```{r}
ironman_lake_placid_female_2022 <- ironman_lake_placid_female_2022[-183, ] 
rownames(ironman_numeric) <- ironman_lake_placid_female_2022$Bib
plot(-prcomp(ironman_numeric, scale=T)$x[,1:2],type="n")
text(-prcomp(ironman_numeric,scale=T)$x[,1], -prcomp(ironman_numeric, scale=T)$x[,2],rownames(ironman_numeric), col = adjustcolor("cyan3"))
```

Los datos que más a la derecha se encuentran y por tanto deberían ser las atletas más rápidas son los números 3 y 5, vamos a analizar los tiempos de estas atletas en cada una de las disciplinas y ver si efectivamente coincide que sean más rápidas:

```{r}
ironman_lake_placid_female_2022[ironman_lake_placid_female_2022$Bib == "3", c("Name", "Run.Time", "Bike.Time", "Swim.Time") ]
#La mas a la derecha
```

```{r}
ironman_lake_placid_female_2022[ironman_lake_placid_female_2022$Bib == "5", c("Name", "Run.Time", "Bike.Time", "Swim.Time") ]
#La mas a la derecha
```

Media de los tiempos de todas las atletas en cada disciplina:

```{r}
pca_result$center
```

Efectivamente, si lo comparamos con la media de todas las corredoras, Sarah True (3) y Rachel Zilinskas (5) tienen tiempos muy por debajo de la media en las tres disciplinas.

Ahora, realizaremos el mismo análisis para el punto que se encuentra más a la izquierda, que corresponde a la atleta 101.

```{r}
ironman_lake_placid_female_2022[ironman_lake_placid_female_2022$Bib == "101", c("Name", "Run.Time", "Bike.Time", "Swim.Time") ]
#La mas a la izquierda
```

Esta vez, los tiempos de esta atleta están muy por encima de la media, por lo que podemos concluir que efectivamente las atletas mas lentas, y por tanto con tiempos mayores, se encuentran mas a la derecha.

Ahora, lo representamos por colores según si ha superado la media en todas las categorías, si no ha superado la media en ninguna y si lo ha superado en algunas si y en otras no, y como podemos observar en el siguiente gráfico coincide con nuestras conclusiones anteriores:

```{r}
# Calcular el PCA
pca_result <- prcomp(ironman_numeric, scale = TRUE)

# Medias de cada disciplina
media_swim <- 88.1032
media_bike <- 424.7561
media_run  <- 324.5799

swim_times <- ironman_lake_placid_female_2022$Swim.Time
bike_times <- ironman_lake_placid_female_2022$Bike.Time
run_times  <- ironman_lake_placid_female_2022$Run.Time

colores <- ifelse(swim_times < media_swim & bike_times < media_bike & run_times < media_run, "slateblue3", 
           ifelse(swim_times > media_swim & bike_times > media_bike & run_times > media_run, "indianred2",    
                  "seagreen3")) 
plot(-pca_result$x[, 1:2], col = colores, pch = 16, 
     xlab = "PC1", ylab = "PC2", main = "PCA según tiempos en Swim, Bike y Run")

# Agregar una leyenda para interpretar los colores
legend("topright", legend = c("Más rápida en todo", "Mixta", "Más lenta en todo"), 
       col = c("slateblue3", "seagreen3", "indianred2"), pch = 16)


```

Realizamos un biplot para ver información sobre las variables principales:

```{r}
biplot(prcomp(ironman_numeric,scale=T)) 
```

La variable PC1 ahora muestra valores negativos a la izquierda y positivos a la derecha, con los tiempos de natación, ciclismo y carrera apuntando hacia valores positivos de PC1. Esto indica que los atletas con tiempos más altos en estas disciplinas se encuentran en la parte derecha del gráfico, mientras que los más rápidos (con tiempos menores) están a la izquierda. Además la corredora Sarah True, que como hemos observado anteriormente es corredora rápida, ahora aparece correctamente en el lado izquierdo del gráfico, lo que cofirma la idea de que tiempos más cortos correspondan a valores más bajos en PC1.

Los vectores de Swim.Time, Bike.Time y Run.Time muestran cómo se relacionan las variables originales con los componentes principales e indican la dirección en la que crecen los valores de cada variable.

Los vectores apuntan hacia la derecha, lo que sugiere que valores más altos en estas variables están asociados con valores más altos en PC1. Los tres vectores están relativamente alineados, lo que significa que las tres variables están correlacionadas: una atleta con un tiempo alto en natación probablemente también tenga tiempos altos en ciclismo y carrera.

Nuestros datos proyectados en el nuevo espacio PCA son:

```{r}
pca_result <- prcomp(ironman_numeric, center = TRUE, scale. = FALSE)
print(head(pca_result$x))
```

Cada fila es una atleta y cada columna (PC1, PC2 Y PC3) una variable del PCA que son combinaciones lineales de las variables originales. Además, un valor más alto o más bajo en PC1 indica que indica que ese atleta está más alejado del promedio en términos de la variabilidad capturada por ese componente.

## Sin usar ninguna función de R específicamente definida para obtener las componentes principales:

**Matriz de covarianza**: El primer paso para realizar un análisis PCA, es calcular la matriz de covarianza.

```{r}
#Seleccionamos solo las variables necesarias para hacer el PCA, que son Swim.Time, Bike.Time y Run.Time
ironman_numeric2 <- ironman_lake_placid_female_2022[, c("Swim.Time", "Bike.Time", "Run.Time")]
cov_matrix <- cov(ironman_numeric2)
cov_matrix
```

**Componentes principales:** El siguiente paso es calcular los autovalores y autovectores usando la matriz de covarianza:

```{r}
#Usamos la función eigen para sacar la descomposición
eigen_decomp <- eigen(cov_matrix)
```

Sacamos autovalores:

```{r}
autovalores <- eigen_decomp$values
autovalores
```

Sacamos autovectores:

```{r}
autovectores <- eigen_decomp$vectors
autovectores
```

Los autovalores y autovectores están ordenados de mayor a menor.

**Selección de componentes principales:**

Anteriormente, habiamos ordenado los autovalores y autovectores de mayor a menor, ya que los primeros componentes principales explican la mayor varianza.

Calcularemos la varianza explicada por cada componente principal diviendo cada autovalor por la suma total de autovalores:

```{r}
varianza_explicada <- autovalores / sum(autovalores)
varianza_explicada

```

La varianza explicada nos muestra el porcentaje de varianza que explica cada componente. Por tanto, la primera componente explica el 83,78% de la varianza, el segundo componente PC2 explica un 14,67% de la varianza y PC3 un 1,5%. Esto ya nos indica que la primera componente principal explica la mayor parte de la varianza y junto con la segunda, ya suman practicamente toda, para verlo aun mas claro, usaremos la varianza acumulada:

```{r}
varianza_acumulada <- cumsum(varianza_explicada)
varianza_acumulada
```

Esto es simplemente la suma acumulada de la varianza explicada y efectivamente, las dos primeras componentes principales explican un 98,45% de la varianza.

```{r}
colores <- c("coral", "brown", "pink")

plot(varianza_acumulada, type="b", xlab="Número de Componentes", 
     ylab="Varianza Acumulada", main="Varianza Acumulada",
     pch=19, col=colores, cex=1.2)

legend("bottomright", legend=paste("PC", 1:length(varianza_acumulada)), 
       col=colores, pch=19, title="Componentes Principales")
```

Usando este gráfico que representa la varianza acumulada por cada componente (que son los puntos) observamos como el primer punto (PC1), explica ya casi un 85% y con el segundo llegan casi a explicar toda la varianza. Si nos fijamos el codo del gráfico (punto donde la pendiente cambia bruscamente) es PC2, por lo que a partir de ahí la siguiente componente no aporta mucha información.

Por tanto, seleccionamos las dos primeras componentes principales, PC1 y PC2, que capturan una cantidad muy significativa de la varianza total.

**Transformación de datos:**

Nuestras variables originales se transforman en un nuevo conjunto de variables, que son las componentes principales, son ortogonales entre sí por lo que no están correlacionadas y capturan la máxima varianza posible de los datos.

Los datos proyectados en el nuevo espacio:

```{r}
#Aquí centramos los datos restando la media de cada variable
datos_centrados <- scale(ironman_numeric2, center = TRUE, scale = FALSE)

#Multiplicamos la matriz centrada por la matriz de autovectores
datos_pca <- datos_centrados %*% autovectores

# Mostrar los primeros valores transformados
print("Datos proyectados en el nuevo espacio PCA:")
print(head(datos_pca))

```

Cada columna es una componente principal y cada fila es una observación, esto es lo equivalente a la operación **(pca_result\$x)** con prcomp() que hemos realizado en el apartado anterior:

```{r}
pca_result <- prcomp(ironman_numeric, center = TRUE, scale. = FALSE)
print(head(pca_result$x))
```

Ambos representan la proyección de los datos en el nuevo espacio y el resultado es el mismo. La única diferencia son los signos de PC1 Y PC2, que son contrarios en la primera y segunda representación, pero esto se debe a que los autovectores pueden tener la dirección invertida sin alterar la varianza explicada ni la relación entre los datos.

Si representamos los datos proyectados en el espacio de PC1 Y PC2 obtenemos:

```{r}
plot(datos_pca[,1:2], xlab="PC1", ylab="PC2", 
     main="Datos proyectados en el espacio de los componentes principales", col = adjustcolor("cyan3"))
```

Comparándolo con la representación de hecha con prcomp():

```{r}
# Eliminar la fila 183 
ironman_numeric <- ironman_numeric[-183, ] 
plot(-prcomp(ironman_numeric, scale=T)$x[,1:2], col = adjustcolor("cyan3"))
```

Las principales diferencias entre las representaciones son dos, la escala de los ejes que se debe a la funcion scale = T de la función prcomp() que ha normalizado las variables antes de calcular el PCA. La otra diferencia es la distribución de los puntos, pero a pesar de no ser iguales están distribuidos de manera similar, esto también puede deberse a la diferencia en el escalado, veamos que ocurre si lo representamos con prcomp() pero sin scale=T:

```{r}
# Eliminar la fila 183 
ironman_numeric <- ironman_numeric[-183, ] 
plot(-prcomp(ironman_numeric)$x[,1:2], col = adjustcolor("cyan3"))
```

Representándolo de esta manera la distribución es exactamente igual que la que hemos obtenido programando a mano. Por tanto, las conclusiones obtenidas del PCA son las mismas ya que hemos llegado al mismo resultado.

```

#Clustering

## Métodos no jerarquicos

Primero, vamos a usar el método de las k-medias.

El método de las k-medias es de los algoritmos de Machine Learning más utilizado para la agrupación de observaciones en un conjunto de k clústeres. Don de k es un valor que hay que fijarlo de antemano.

Antes de empezar a hacer las k-medias, dado que es un algoritmo que solo admite valores tipo numéricos, vamos a crear una nueva variable para que solo tenga como columnas variables de tipo numérico y vamos a escalarlo.

```{r}
train_num <- train %>% select(Age, Height, Weight, FCVC, NCP, CH2O, FAF, TUE)
train_esc <- scale (train_num)
```

### Número óptimo de clusters

Para ver el número óptimo de clusters, vamos a usar 4 métodos: el método del codo, método de la silueta, método de Gap y usando la función NbClust.

**- El método del codo**

Según este método, el número mas óptimo de clusters es cuando se da un cambio abrupto en la pendiente de la función

```{r}
fviz_nbclust(train_esc, kmeans, method = "wss")
```

Analizando la gráfica que nos da, podemos decir que en **el número óptimo de clusters necesarios sería 5** si usamos el método del codo.

**- Método de la silueta**

Este método nos ayuda a valorar la coherencia (o calidad) de los resultados de los clusters, es decir, determina hasta qué punto cada observacion se encuentra correctamente ubicada dentro de su agrupación.

Este algoritmo nos señala con una línea discontinua vertical el número mas óptimo

```{r}
fviz_nbclust(train_esc, kmeans, method = "silhouette")
```

Este algoritmo nos dice que **el número óptimo de clusters es 9**. 

Viendo el gráfico y las conclusiones que sacamos de él, no es muy difícil ver que no tiene mucho sentido escoger 9 grupos, ademas de la complejidad que tendría cada grupo y lo difícil que sería diferenciarlo de los demás grupos.

De todas formas, para confirmar lo que decimos, vamos a comprobar si haciendo el algoritmo de las k-medias con 9 centros (9 clusters) nos puede dar una buena agrupacion interpretando el gráfico de siluetas. 

```{r}
# k-medias con 9 centros
k9 <- kmeans(train_esc, centers = 9, nstart = 50)

# Gráfica de siluetas
sil <- silhouette(k9$cluster, dist (train_esc))
fviz_silhouette(sil)
```

Para interpretar el gráfico, hay que fijarse en los valores positivos y negativos. Aquellos que sean mas próximos a 1, es que la observación está bien agrupada, mientras que cuando esté mas cerca de 0 es que está mal agrupada. 

Ahora, analizando la gráfica que tenemos, podemos concluir que muchos de los grupos tienen valores negativos o cerca de 0, por lo que eso es que no es una buena agrupación a pesar de que uno de los grupos cuyos valores son todos positivos (y muchos de ellos llegando a sobrepasar 0.5) es una muy buena aproximación.

Es muy fácil ver que **una agrupacion de las observaciones en 9 clusters no es una buena opcion**. 


**- Gap**

Es un método que determina el número de clusters sin depender de suposiciones y así permitiendo tomar decisiones mas objetivas.

```{r}
gap_stat <- clusGap(train_num, FUN = kmeans, nstart = 25,
                    K.max = 15, B = 50)

print(gap_stat, method = "firstmax")

fviz_gap_stat(gap_stat)
```

**7 clusters parece ser el número óptimo segun el método de Gap** para los datos que tenemos.

**- NbClust**

Vamos a terminar con este método que nos proporciona 30 índices para determinar el número relevate de clusters, ya que cada método que hemos probadoa hasta ahora nos da valores distintos para agrupar los datos.

```{r}
nb <- NbClust(train_esc, distance = "euclidean", min.nc = 2, max.nc = 12, method = "kmeans")
```

Al parecer, el número más óptimo es 2 ó 5, con 6 indices cada uno que los respalda. Vamos a ver la explicabilidad con los resultados obtenidos.

```{r}
# Con 5 clusters
res_kmeans_n5 <-  cluster_analysis(train_esc, n = 5, method = "kmeans")
plot (summary (res_kmeans_n5))

# Con 2 clusters
res_kmeans_n2 <-  cluster_analysis(train_esc, n = 2, method = "kmeans")
plot (summary (res_kmeans_n2))
```

```{r}
k5 <- kmeans(train_esc, centers = 5, nstart = 50)

fviz_cluster(k5, data = train_esc, geom = "point")
```

Nos quedamos con 5 clusters porque diferencian más los grupos.

## Métodos jerárquicos

### Cluster Jerárquico Aglomerativo

Primero vamos a entender este tipo de cluster jerárquico.

Se empieza con n conglomerados, uno por cada dato (en nuestro caso, n = 2111) y en cada paso se van fusionando los 2 grupos más similares, hasta que llega a un punto en que se queda en un solo grupo que contiene a todos.

Primero vamos a ver el coeficiente aglomerativo. Cuanto mas cercano a 1, más fuerte es el agrupamiento. Para tener más opiniones sobre qué tipo de método usar, vamos a ver el coeficiente aglomerativo de 4 métodos: Average, Single, Complete, Ward

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# Función para calcular el coeficiente de agrupamiento
ac <- function(x) {
  agnes(train_esc, method = x)$ac
}

map_dbl(m, ac)
```

Dado que el coeficiente más cercano a 1 es el método Ward con un 0.9843. 

```{r}
d <- dist(train_esc, method = "euclidean")

hc1 <- hclust(d, method = "ward" )

# Dendrograma
plot(hc1, cex = 0.6, hang = -1)
```

```{r}
# Cortamos en 5 clusters
sub_grp <- cutree(hc1, k = 5)

# Visualizamos el corte en el dendrograma
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 5, border = 2:5)
```

```{r}
fviz_cluster(list(data=train_esc,cluster=sub_grp), geom = "point")
```
