---
title: "Funciones"
author: "Carmen Liberal, Marcos López, Lara Montero"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    toc: yes
    toc_float:
      collapsed: true
---

```{r echo=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(readr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(HDclassif))
suppressPackageStartupMessages(library(MASS))
suppressPackageStartupMessages(library(gt))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(cluster))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library (NbClust))
suppressPackageStartupMessages(library (parameters))
suppressPackageStartupMessages(library (stats))
suppressPackageStartupMessages(library(summarytools))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(rpart.plot))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(naivebayes))
```

```{r}
# Leer el dataset correctamente
obesidad <- read.csv("C:/Users/carme/Downloads/ObesityDataSet.csv")

# Ver las dimensiones del dataset
data <- dim(obesidad)

```

```{r echo = FALSE}

obesidad$Age <- round(obesidad$Age)
obesidad$NCP <- round(obesidad$NCP)
obesidad$FAF <- round(obesidad$FAF)

set.seed(1)

# Indices para la partición
nobesidad <- dim(obesidad)[1]
indices <- 1:nobesidad
ntrain <- nobesidad * 0.6
indices.train <- sample(indices, ntrain, replace = FALSE)
indices.test_val <- indices[-indices.train]

# Usamos el 60% para las variables de entrenamiento
train <- obesidad[indices.train, ]
test_val <- obesidad[indices.test_val, ]

# Creamos los indices para test y para val
ntest_val <- dim(test_val)[1]
indices = 1:ntest_val
ntest <-  ntest_val * 0.5
indices.test <- sample(indices, ntest, replace = FALSE)
indices.val <- indices[-indices.test]

# Creamos los datos de test y val
test <- test_val[indices.test, ]
val <- test_val[indices.val, ]


```

# k-NN
# Función k-NN con distancia euclides
mi_knn <- function(train, test, cl, k = 5) {
  # Verificamos que las dimensiones estan bien
  if(nrow(train) != length(cl)) {
    stop("El número de filas de train debe coincidir con la longitud de cl")
  }
  
  # Convertir a matrices para evitar problemas con data frames
  train <- as.matrix(train)
  test <- as.matrix(test)
  cl <- as.vector(cl)
  
  # Comprobamos que train y test tienen el mismo número de columnas
  if(ncol(train) != ncol(test)) {
    stop("train y test deben tener el mismo número de columnas")
  }
  
  # Calculamos las distancias euclideas
  distancias <- sqrt(outer(
    rowSums(train^2), 
    rowSums(test^2), 
    "+"
  ) - 2 * tcrossprod(train, test))
  
  # Buscamos lo k, vecinos más cercanos
  vecinos <- apply(distancias, 2, function(x) order(x)[1:k])
  
  # Despues de buscarlo predecimos las clases más frecuentes.
  predicciones <- apply(vecinos, 2, function(indices) {
    clases_vecinos <- cl[indices]
    names(which.max(table(clases_vecinos)))
  })
  
  return(predicciones)
}

# Análisis Discriminante Lineal.

# Bagging

Comenzamos detallando los pasos que hemos seguido para programar el método de bagging.

1.  Definimos manualmente los niveles posibles de cada variable categórica ya que algunos niveles pueden no aparecer en las muestras aleatorias y da error al predecir. También recorremos todas las columnas de train y ajustamos los niveles categóricos según la lista que ya hemos creado.

2.  Función bagging_manual() para el entrenamiento. Creamos una función que entrena un conjunto de árboles de decision (rpart) usando resampling con reemplazo:

    · En cada iteración creamos una muestra aleatoria del conjunto de entrenamiento.

    · Se entrena un árbol con esa muestra

    · Almacenamos tambien los índices OOB que son las observaciones no utilizadas por ese árbol.

    Repetimos para 500 y árboles y el resultado es una lista de modelos y sus datos OOB.

3.  Función prediccion_oob para la predicción

    · Con esta función predecimos cada observación de train usando solo árboles para los que esa observación fue OOB.

    · Cada observación recibe una serie de votos (predicciones de diferentes árboles) y se aplica la votación mayoritaria para asignar la clase final.

    · Esto simula el cálculo del error OOB.

4.  Evaluamos el modelo bagging manual en el archivo principal generando la matriz de confusion con las predicciones OOB frente a la etiquetas del conjunto train y comparandolo con la funcion propia de R.

#### Definimos los niveles manualmente

```{r}
# Creamos lista con los niveles correctos para todas las variables categóricas
niveles_fijos <- list(
  Gender = c("Female", "Male"),
  family_history_with_overweight = c("yes", "no"),
  FAVC = c("yes", "no"),
  CAEC = c("no", "Sometimes", "Frequently", "Always"),
  SMOKE = c("yes", "no"),
  SCC = c("yes", "no"),
  CALC = c("no", "Sometimes", "Frequently", "Always"),
  MTRANS = c("Public_Transportation", "Walking", "Bike", "Motorbike", "Automobile"),
  NObeyesdad = levels(train$NObeyesdad)
)

#Recorremos todas las columnas del dataset y si en niveles_fijos hay niveles definidos los aplica ahí

for (col in names(train)) {
  if (!is.null(niveles_fijos[[col]])) {
    train[[col]] <- factor(train[[col]], levels = niveles_fijos[[col]])
  }
}

```

#### Entrenamos el modelo bagging

```{r}

#Creamos esta función que entrena un modelo con bagging usando n_trees arboles
bagging_manual <- function(data, target, n_trees = 500)

{
  
  modelos <- list()     # Lista para guardar los árboles entrenados
  oob_indices <- list() # Lista para guardar los índices OOB de cada árbol
  set.seed(123)         #Para que los resultados sean reproducibles
  
  #Repetimos este bucle n_trees veces
  for(i in 1:n_trees) {
    n <- nrow(data)
    
    #Creamos una muestra con reemplazo
    muestra_idx <- sample(1:n, size = n, replace = TRUE)
    muestra <- data[muestra_idx, ]
    
    #Identificamos las observaciones que no han sido usadas. Estos son los datos   OOB que se usaran despues como validacion interna
    oob_idx <- setdiff(1:n, unique(muestra_idx))
    
    # Ajustamos los niveles de las variables categóricas para asegurarnos de que no falte ningun valor
  for (col in names(muestra)) {
   if (!is.null(niveles_fijos[[col]])) {
    muestra[[col]] <- factor(muestra[[col]], levels = niveles_fijos[[col]])
  }
}

    
    #Creamos la fórmula del modelo:
    formula <- as.formula(paste(target, "~ ."))
    
    #Entrenamos un árbol de decisión con rpart
    arbol <- rpart(formula, data = muestra, method = "class")
    
    #Guardamos el árbol y los indices OOB
    modelos[[i]] <- arbol
    oob_indices[[i]] <- oob_idx
    
  }
  # Devolvemos todos los árboles y sus OOB correspondientes
  return(list(modelos = modelos, oob = oob_indices))
  
}
```

#### Predicción con observaciones Out Of the Bag

```{r}

#Creamos una función que predice la clase de cada observacion usando solo los arboles donde esa observacion fue OOb
prediccion_oob <- function(modelos, oob_indices, data) {
  
   #Crea una lista vacía para guardar los "votos" que recibe cada observación (de  los árboles que la dejaron fuera al entrenar)
  n <- nrow(data)
  votos <- vector("list", n) # Guardaremos los votos para cada observación
  
  #Recorremos cada arbol y su correspondiente conjunto OOB
  for (i in seq_along(modelos)) {
    arbol <- modelos[[i]]
    oob <- oob_indices[[i]]
    #Extraemos los datos OOb para este arbol
    if(length(oob) > 0) {
            datos_oob <- data[oob, ]

      # Ajustamos los niveles para que coincidan con los del entrenamiento
      for (col in names(datos_oob)) {
        if (!is.null(niveles_fijos[[col]])) {
          datos_oob[[col]] <- factor(datos_oob[[col]], levels = niveles_fijos[[col]])
        }
      }
      # Hacemos la prediccion para las observaciones OOb usando el arbol actual
      pred <- predict(arbol, datos_oob, type = 'class')

      
      # Guardamos los votos por cada observación
      for (j in seq_along(oob)) {
        idx <- oob[j]
        votos[[idx]] <- c(votos[[idx]], as.character(pred[j]))
    }
  }
  }
    # Para cada observación, hacemos votación mayoritaria
   pred_final <- sapply(votos, function(v) {
    if (length(v) == 0) return(NA)  # Si no recibió ningún voto, lo dejamos en NA
    names(sort(table(v), decreasing = TRUE))[1]  # Clase más votada
  })
  
  return(factor(pred_final, levels = niveles_fijos$NObeyesdad))

}
  
  
```

# Implementación de Boosting (versión Adaboost)

Primero vamos a explicar los pasos:

1.  Inicializamos los pesos de las observaciones:

$$
w_i=1/n, i=1,\dots,n
$$

2.  Bucle para $m = 1,...,M$. Para cada iteración $m$ realizamos los siguientes pasos:

-   Ajustamos el clasificador $f_m(x)$ al conjunto de entrenamiento usando los pesos $w_i$. Este clasificador $f_m(x, \phi_m)$ es un modelo base que depende de los parámetros $\phi_m$
-   Calcular el error ponderado de $f_m(x)$ (este error nos indica lo bien o mal que está funcionando el clasificador):

$$
error_{m}=\frac{\sum_{i=1}^{n}w_i I(y_i \neq f_{m}(\mathbf{x}_i))}{\sum_{i=1}^{n}w_i},
$$

donde $I(y_i \neq f_{m}(\mathbf{x}_i))$ es la función indicadora donde toma valor 1 si la predicción es incorrecta y 0 en caso contrario. - Una vez con el error, calculamos el coeficiente $\alpha_m$, que mide la importancia del clasificador en la actualización del modelo acumulado.

$$
\alpha_m = log\left(\frac{1-error_m}{error_m} \right)
$$

-   Actualizar los pesos de las observaciones; por cada observación mal clasificada, le asignamos un aumento de peso. Con esto conseguimos que el siguiente clasificador se enfoque más en estas observaciones. Los nuevos pesos $w_i^{(m+1)}$ se calculan de la siguiente forma:

$$
w_i^{(m+1)} =w_i^{(m)} exp(\alpha_m I(y_i \neq f_{m}(\mathbf{x}_i))), i=1,\dots, n,
$$ donde $y_i$ es la observación real y $exp(\alpha_m I(y_i \neq f_{m}(\mathbf{x}_i)))$ aumenta el peso de las observaciones mal clasificadas.

-   Por último, el modelo final $F(x)$ es la combinación de los clasificadores base ajustados en cada interación ponderados por $\alpha_m$, es decir:

$$
F(\mathbf{x})=sign\left( \sum_{m=1}^{M} \alpha_{m} f_{m}(\mathbf{x})\right)
$$

Ahora que tenemos las fórmulas y entendemos el procedimiento, vamos a implementarlo a mano.

```{r}
copy_train <- train
copy_test <- test
copy_train$y <- as.numeric(as.factor(copy_train$NObeyesdad)) - 1
copy_test$y <- as.numeric(factor(copy_test$NObeyesdad, levels = levels(as.factor(copy_train$NObeyesdad)))) - 1

# Número de clases
levels_lbl <- levels(as.factor(copy_train$NObeyesdad))
K <- length(levels_lbl)

# Pasar de factores a numéricos
tabla_preds <- setdiff(names(copy_train), c("NObeyesdad", "y"))
copy_train[tabla_preds] <- lapply(copy_train[tabla_preds], function(z) if(is.numeric(z)) z else as.numeric(as.factor(z)))
copy_test[tabla_preds]<- lapply(copy_test[tabla_preds],  function(z) if(is.numeric(z)) z else as.numeric(as.factor(z)))

df_train <- copy_train[c(tabla_preds, "y")]
df_train$y <- factor(df_train$y, levels = 0:(K-1))
df_test <- copy_test[tabla_preds]

# Función AdaBoost
adaboost_train_rpart <- function(df, M) {
  n <- nrow(df)
  w <- rep(1/n, n)
  stumps <- vector("list", M)
  alphas <- numeric(M)
  K <- length(levels(df$y))
  
  for (m in seq_len(M)) {
    stump <- rpart(y ~ ., data = df, weights = w, control = rpart.control(maxdepth = 1, cp = 0, minsplit = 2))
    pred_factor <- predict(stump, df, type = "class")
    pred_num <- as.numeric(pred_factor) - 1
    y_num <- as.numeric(df$y) - 1
    
    # Error ponderado
    err <- sum(w * (pred_num != y_num))
    err <- pmin(pmax(err, 1e-10), 1 - 1e-10)
    
    # Coeficiente alpha
    alphas[m] <- log((1 - err)/err) + log(K - 1)
    
    # Actualizar pesos
    w <- w * exp(alphas[m] * (pred_num != y_num))
    w <- w / sum(w)
    
    stumps[[m]] <- stump
  }
  list(stumps = stumps, alphas = alphas, K = K)
}

# Función de predicción
adaboost_predict_rpart <- function(model, df) {
  M <- length(model$alphas)
  n <- nrow(df)
  scores <- matrix(0, n, model$K)
  
  for (m in seq_len(M)) {
    pred_factor <- predict(model$stumps[[m]], df, type = "class")
    pred_num <- as.numeric(pred_factor) - 1
    for (k in 0:(model$K - 1)) {scores[, k+1] <- scores[, k+1] + model$alphas[m] * (pred_num == k)}
  }
  
  apply(scores, 1, which.max) - 1
}

# Tiempo
time_taken <- system.time({model_ab <- adaboost_train_rpart(df_train, M = 10)})
cat(sprintf("Tiempo de entrenamiento: %.2f s\n", time_taken[3]))

# Predicciones y accuracy
datos_pred <- df_test
preds <- adaboost_predict_rpart(model_ab, datos_pred)
accuracy <- mean(preds == copy_test$y) * 100
cat(sprintf("Accuracy en test: %.2f%%\n", accuracy))
```

Pasos:

1.  Inicializar los pesos $w_i = 1/n$.
2.  Para $m = 1, ..., M$ :

-   Entrenamos un árbol de decisión de profundidad 1 (stump) con los pesos $w$.
-   Calculamos el error ponderado.
-   fijamos el coeficiente $\alpha_m$.
-   Actualizamos y normalizamos los pesos.

3.  Predicciones: para cada clase k, se suman los $\alpha_m$ de los arboles que predicen k y escogemos la clase con mayor suma.

En el código, la función `adaboost_train_rpart()` hacemos los bucles, pesos y se guardan los árboles y valores de alpha, mientras que `adaboost_predict_rpart()` es la que elige la clase "ganadora".

# Naive Bayes

Antes de comenzar con el código vamos a detallar los pasos que seguimos para realizar manualmente el método Naive Bayes:

1.  Entrenamos el modelo manual construyendo la función entrenar_naive_bayes() que:

    · Recorre cada clase de obesidad del target N0obeyesdad y calcula su probabilidad a priori (nrow(subconjunto) / total)

    · Para cada predictor calcula la probabilidad condicional de cada valor dado cada clase usando suavizado Laplace.

    · Almacenamos en una lista toda esta información organizada por clases

2.  Predicción con Naive Bayes con la función predecir_naive_bayes()

    · Para cada observación calculamos la probabilidad total (en escala logaritmica) para cada clase de obesidad.

    · Sumamos los logaritmos de las probabilidades condicionales de cada atributo dado una clase.

    · Nos devuelve la clase más probable según el criterio de máxima verosimilitud

3.  Evaluación del modelo

    · Aplicamos el modelo a test prediciendo la clase de obesidad para cada observacion.

    · Comparamos las clases predichas con las reales, construyendo la matriz de confusión, la cual muestra cuantas veces el modelo acertó o se equivocó al predecir cada clase, a partir de la matriz ontenemos distintas métricas.

#### Entrenamienro del modelo

```{r}
# Esta funcion entrena el modelo Naive Bayes usando conteos y probabilidales con el suavizado de Laplace
#Definimos una funcion que recibe el conjunto de datos, y el target
entrenar_naive_bayes <- function(data, target_col) {
 
   #Extraemos los valores unicos de la variable objetivo
  clases <- unique(data[[target_col]])
  
  #Guardamos el total de filas del dataset
  total <- nrow(data)
  
  #Guardamos todas las probabilidades calculadas para cada clase
  modelo <- list()
  
  #Recorremos cada clase posible (en nuestro caso de obesidad):
  for (clase in clases) {
  
      #Filtramos solos las filas que pertenecen a esa clase:
    subconjunto <- data[data[[target_col]] == clase, ]
    
    #Probabilidad a prori de esa clase (numero de ocurrencias entre el total):
    prob_clase <- nrow(subconjunto) / total
    
    #Guardamos la probabilidades condicionales para cada atributo:
    probs_atributos <- list()
    
    #Recorremos todas las columnas excepto la clase que vamos a predecir:
    for (col in colnames(data)) {
    
        if (col != target_col) {
      
            #Creamos una tabla de frecuencias para esa columna solo con las filas de la clase actual:
        tabla <- table(subconjunto[[col]])
        
        #Aplicamos el suavizado de Laplace que suma 1 a cada frecuencua para evitar ceros
        probs <- (tabla + 1) / (sum(tabla) + length(unique(data[[col]])))
        
        #Guardamos las probabilidades condicionales de ese atributo para esta clase
        probs_atributos[[col]] <- probs
      }
    }
    
    #Guardamos en la lista del modelo la probabilidad a prioti y las condcionales para cada variable
    modelo[[as.character(clase)]] <- list(
      prob = prob_clase,
      atributos = probs_atributos
    )
  }
  
  modelo$target <- target_col
  return(modelo)
}


```

#### Predicción con Naive Bayes

```{r}
# Función para predecir una fila con el modelo entrenado
#Predice la clase para una nueva observacion usando log-probabilidades
predecir_naive_bayes <- function(modelo, nueva_obs) {
  target <- modelo$target
  clases <- names(modelo)[names(modelo) != "target"]
  
  #Donde almacenamos el log-probabilidad total para cada clase
  log_probs <- c()
  
  #Empieza a recorrer clases y toma la log de la probabilidad a priori
  for (clase in clases) {
    log_prob <- log(modelo[[clase]]$prob)
    
    #Recorremos cada variable de la observacion y tomamos su valor
    for (col in names(nueva_obs)) {
      valor <- as.character(nueva_obs[[col]])
      
      #Busca la probabilidad condicional de ese valor dado la clase
      if (col %in% names(modelo[[clase]]$atributos)) {
        probs_col <- modelo[[clase]]$atributos[[col]]
        prob_valor <- probs_col[valor]
        
        #Si ese valor no existia en el entrenamiento aplicamos Laplace para asignarle una probabilidad pequeña
        if (is.na(prob_valor)) {
          prob_valor <- 1 / (sum(probs_col) + length(probs_col))  # Laplace para valor desconocido
        }
        #Acumula la log-probabilidad total
        log_prob <- log_prob + log(prob_valor)
      }
    }
    #Guardamos la log-probabilidad total para esa clase
    log_probs[clase] <- log_prob
  }
  #Devuelve la clase con mayor log-probabilidad
  clase_predicha <- names(which.max(log_probs))
  return(clase_predicha)
}


```

#### Evaluación del modelo

```{r}
# Aseguramos niveles iguales entre predicciones y etiquetas reales
niveles_correctos <- levels(as.factor(train$NObeyesdad))  # niveles del train que entrenó el modelo

pred_test_manual <- factor(pred_test_manual, levels = niveles_correctos)
test$NObeyesdad <- factor(test$NObeyesdad, levels = niveles_correctos)

# Predicciones en TEST
pred_test_manual <- sapply(1:nrow(test), function(i) {
  predecir_naive_bayes(modelo_naive_manual, test[i, ])
})

# Evaluación
confusion_test <- confusionMatrix(factor(pred_test_manual, levels = levels(test$NObeyesdad)),
                                  test$NObeyesdad)
print(confusion_test)

```
